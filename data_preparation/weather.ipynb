{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect( \\\n",
    "    database=\"thesis\",\n",
    "    user = \"postgres\", \n",
    "    password = \"jonp8UMs8qDV4jEcwOC0\",\n",
    "    host = \"localhost\"\n",
    "    )\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis_path = r\"C:\\Users\\cvaka\\OneDrive\\Master\\Thesis\"\n",
    "#thesis_path = r\"C:\\Users\\Christophe\\OneDrive\\Master\\Thesis\"\n",
    "max_entries = 10000\n",
    "table = 'wfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = pd.read_csv(thesis_path+\"/data/weather/WFS_tables_dump/wfs_values_raw.csv\")\n",
    "\n",
    "df_values.rename(columns= {\n",
    "    \"WEATHERFORECASTSCHIPHOL_ID\": \"id\",\n",
    "    \"ROWNUMBER\": \"row\",\n",
    "    \"DATA\": \"data\"\n",
    "}, inplace=True)\n",
    "\n",
    "row_dict = {\n",
    "    1005: \"t\",\n",
    "    1006: \"f_type\", # Forecast type\n",
    "    2001: \"rvr5000_1000\",\n",
    "    2003: \"rvr1500_300\",\n",
    "    2004: \"rvr550_200\",\n",
    "    2005: \"rvr350\",\n",
    "    2011: \"rvrcat\",\n",
    "    3001: \"wind_dir\",\n",
    "    3002: \"wind_dir_std\",\n",
    "    3011: \"wind_spd\",\n",
    "    3012: \"wind_spd_std\",\n",
    "    3021: \"wind_stoten\",\n",
    "    4001: \"temp\",\n",
    "    4011: \"dew\",\n",
    "    5001: \"snow\",\n",
    "    5002: \"snow_heavy\",\n",
    "    5011: \"rain_cool\",\n",
    "    5021: \"cb\",\n",
    "    5031: \"lightning\",\n",
    "    6001: \"rvr5000_2000\"\n",
    "}\n",
    "\n",
    "df_values = df_values[df_values[\"row\"].isin(row_dict)]\n",
    "df_values['row'].replace(row_dict, inplace=True)\n",
    "df_values['data'] = df_values['data'].str.split(' ')\n",
    "\n",
    "df_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(thesis_path+\"/data/weather/WFS_tables_dump/wfs_meta_raw.csv\", \"r\") as f:\n",
    "    df_meta = pd.read_csv(StringIO(re.sub(r'(\\d{1,2}\\/\\d{1,2}\\/\\d{4}),', r'\\1 00:00:00,', f.read())))\n",
    "\n",
    "df_meta.rename(columns= {\n",
    "    \"WEATHERFORECASTSCHIPHOL_ID\": \"id\",\n",
    "    \"BEGIN_TIME_FORECAST_SHORT\": \"t_start_short\",\n",
    "    \"PUBLICATION_TIME_FORECAST_SHOR\": \"t_publ_short\",\n",
    "    \"BEGIN_TIME_FORECAST_LONG\": \"t_start_long\",\n",
    "    \"PUBLICATION_TIME_FORECAST_LONG\": \"t_publ_long\",\n",
    "    \"VALID_FOR_PERIOD\": \"fluff\"\n",
    "}, inplace=True)\n",
    "df_meta = df_meta[df_meta['id'] >= 37882] # Select 2017 onwards, when using the full set the program will fail due to inconsistent reporting of snow in weather IDs 2888 tot 4705\n",
    "# df_meta = df_meta.astype({\n",
    "#     \"id\": int,\n",
    "#     \"t_start_short\": \"datetime64[ns]\",\n",
    "#     \"t_publ_short\": \"datetime64[ns]\",\n",
    "#     \"t_start_long\": \"datetime64[ns]\",\n",
    "#     \"t_publ_long\": \"datetime64[ns]\",\n",
    "#     })\n",
    "timecols = ['t_start_short', 't_publ_short', 't_start_long', 't_publ_long']\n",
    "for col in timecols:\n",
    "    df_meta[col] = pd.to_datetime(df_meta[col], format= '%d/%m/%Y %H:%M:%S')\n",
    "df_meta.drop([\"fluff\"], axis=1, inplace=True)\n",
    "df_meta.reset_index(drop=True, inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_out = []\n",
    "failed = []\n",
    "\n",
    "pd.set_option(\"mode.chained_assignment\", None)\n",
    "\n",
    "for i, meta in df_meta.iterrows():\n",
    "    values = df_values[df_values['id']==meta['id']]\n",
    "    values = values[~values['data'].isna()]\n",
    "    if len(values) == 0:\n",
    "        failed.append(\"No data found for weather ID {}\".format(meta['id']))\n",
    "    elif not 't' in values[['row']].values:\n",
    "        failed.append(\"No time data found for weather ID {}\".format(meta['id']))\n",
    "    elif not 'f_type' in values[['row']].values:\n",
    "        failed.append(\"No forecast time found for weather ID {}\".format(meta['id']))\n",
    "    else:\n",
    "        values = pd.DataFrame(np.array(list(values['data'])).transpose(), columns=values['row'])\n",
    "        values['t_publish'] = pd.to_datetime( \\\n",
    "        (values['f_type']=='k') * ((meta['t_publ_short'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')) + \\\n",
    "        (values['f_type']=='l') * ((meta['t_publ_long'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')), unit='s')\n",
    "        values['t_test'] = values['t'].copy()\n",
    "        values['t'][1:] = (values['t'].astype(int)-values['t'].astype(int).shift())[1:] # Calculate difference with previous hour\n",
    "        values['t'][0] = int(values['t'][0]) - meta['t_start_short'].hour # Calculate hour difference with meta['t_start_short']\n",
    "        values['t'] = meta['t_start_short'] + pd.to_timedelta((values['t'] + 24*(values['t']<0)).cumsum(), unit='h')\n",
    "\n",
    "        values.sort_values('t', inplace=True) # Probably not necessary\n",
    "\n",
    "        values['valid_from'] = pd.to_datetime((values['t'].astype('int64') + values['t'].astype('int64').shift()) / 2)\n",
    "        values['valid_from'].iloc[0] = values['t'].iloc[0] - pd.Timedelta(30, 'm') * int(values['f_type'].iloc[0]=='k') - pd.Timedelta(90, 'm') * int(values['f_type'].iloc[0]=='l')\n",
    "\n",
    "        values['valid_till'] = pd.to_datetime((values['t'].astype('int64') + values['t'].astype('int64').shift(-1)) / 2)\n",
    "        values['valid_till'].iloc[-1] = values['t'].iloc[-1] + pd.Timedelta(30, 'm') * int(values['f_type'].iloc[-1]=='k') + pd.Timedelta(90, 'm') * int(values['f_type'].iloc[-1]=='l')\n",
    "\n",
    "        values['forecast_id'] = meta['id']\n",
    "        data_out.append(values)\n",
    "    if i % int(len(df_meta)/100) == 0:\n",
    "        pct = int(i/int(len(df_meta)/100))\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"\\t[%-20s] %d%%\" % ('='*int(pct/5), pct))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "data_out = pd.concat(data_out)\n",
    "\n",
    "for i in failed:\n",
    "    print(i)\n",
    "\n",
    "if len(data_out[data_out['t_test'].astype(int)!= data_out['t'].dt.hour]):\n",
    "    raise Exception(\"\"\"Data Inconsistencies for: \n",
    "    {}\"\"\".format(data_out[data_out['t_test'].astype(int)!= data_out['t'].dt.hour]))\n",
    "\n",
    "if len(data_out[data_out.duplicated(subset=['forecast_id', 't'], keep=False)]):\n",
    "    raise Exception(\"\"\"Primary Key Violation\n",
    "    {}\"\"\".format(data_out[data_out.duplicated(subset=['forecast_id', 't'], keep=False)]))\n",
    "    \n",
    "pd.set_option(\"mode.chained_assignment\", \"warn\")\n",
    "\n",
    "# 237 s\n",
    "# values[['t_publish', 'f_type', 't', 'valid_from', 'valid_till']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_range\n",
    "data_out['valid_range'] = '[' + data_out['valid_from'].astype(str) + ', ' + data_out['valid_till'].astype(str) + ')'\n",
    "# wind_stoten\n",
    "pd.set_option(\"mode.chained_assignment\", \"warn\")\n",
    "data_out['wind_stoten'] = ((data_out['wind_stoten']==\"-99\").astype(int) * (data_out['wind_spd']) + \\\n",
    "                                (data_out['wind_stoten']!=\"-99\").astype(int) * (data_out['wind_stoten']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter & store\n",
    "data_out = data_out[(data_out[['t','t_publish']] >= \"01-01-2005\").all(axis=1)]\n",
    "data_out = data_out.reindex(columns=[\"forecast_id\", 't_publish', 'valid_range']+list(row_dict.values()))\n",
    "data_out.to_csv(\"data_out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "# DROP TABLE IF EXISTS {table};\n",
    "\n",
    "# CREATE TABLE {table} (\n",
    "#     forecast_id bigint,\n",
    "#     t_publish timestamptz,\n",
    "#     valid_range tstzrange,\n",
    "#     t timestamptz,\n",
    "#     f_type text,\n",
    "#     rvr5000_1000 bigint,\n",
    "#     rvr1500_300 bigint,\n",
    "#     rvr550_200 bigint,\n",
    "#     rvr350 bigint,\n",
    "#     rvrcat text,\n",
    "#     wind_dir bigint,\n",
    "#     wind_dir_std bigint,\n",
    "#     wind_spd bigint,\n",
    "#     wind_spd_std bigint,\n",
    "#     wind_stoten bigint,\n",
    "#     temp bigint,\n",
    "#     dew bigint,\n",
    "#     snow bigint,\n",
    "#     snow_heavy bigint,\n",
    "#     rain_cool bigint,\n",
    "#     cb bigint,\n",
    "#     lightning bigint,\n",
    "#     RVR5000_2000 bigint,\n",
    "#     PRIMARY KEY (forecast_id, t)\n",
    "# );\n",
    "\n",
    "# CREATE INDEX {table}_t_publish\n",
    "#     ON public.{table} USING btree\n",
    "#     (t_publish ASC NULLS LAST)\n",
    "# ;\n",
    "\n",
    "# CREATE INDEX {table}_t\n",
    "#     ON public.{table} USING btree\n",
    "#     (t ASC NULLS LAST)\n",
    "# ;\n",
    "\n",
    "# CREATE INDEX {table}_valid_range\n",
    "#     ON public.{table} USING gist\n",
    "#     (valid_range)\n",
    "# ;\n",
    "\n",
    "# CREATE INDEX {table}_forecast_id\n",
    "#     ON public.{table} USING btree\n",
    "#     (forecast_id ASC NULLS LAST)\n",
    "# ;\n",
    "# \"\"\".format(table=table)\n",
    "# cur.execute(query)\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "set PGPASSWORD=jonp8UMs8qDV4jEcwOC0\n",
    "\"C:\\\\Program Files\\\\PostgreSQL\\\\13\\\\bin\\\\psql.exe\" thesis postgres\n",
    "\\copy public.{table} (forecast_id, t_publish, valid_range, t, f_type, rvr5000_1000, rvr1500_300, rvr550_200, rvr350, rvrcat, wind_dir, wind_dir_std, wind_spd, wind_spd_std, wind_stoten, temp, dew, snow, snow_heavy, rain_cool, cb, lightning, RVR5000_2000) FROM 'C:/Users/cvaka/OneDrive/Master/Thesis/SQL/WEATHER/data_out.csv' DELIMITER ',' CSV HEADER QUOTE '\"' ESCAPE '''';\n",
    "\"\"\".format(table=table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out[data_out['forecast_id']==43423][['forecast_id', 't_publish', 'valid_range', 't']]\n",
    "# 43423"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DST voor Ferdinand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(thesis_path+\"/data/weather/WFS_tables_dump/wfs_meta_raw.csv\", \"r\") as f:\n",
    "    df_meta = pd.read_csv(StringIO(re.sub(r'(\\d{1,2}\\/\\d{1,2}\\/\\d{4}),', r'\\1 00:00:00,', f.read())))\n",
    "\n",
    "df_meta.rename(columns= {\n",
    "    \"WEATHERFORECASTSCHIPHOL_ID\": \"id\",\n",
    "    \"BEGIN_TIME_FORECAST_SHORT\": \"t_start_short\",\n",
    "    \"PUBLICATION_TIME_FORECAST_SHOR\": \"t_publ_short\",\n",
    "    \"BEGIN_TIME_FORECAST_LONG\": \"t_start_long\",\n",
    "    \"PUBLICATION_TIME_FORECAST_LONG\": \"t_publ_long\",\n",
    "    \"VALID_FOR_PERIOD\": \"fluff\"\n",
    "}, inplace=True)\n",
    "timecols = ['t_start_short', 't_publ_short', 't_start_long', 't_publ_long']\n",
    "for col in timecols:\n",
    "    df_meta[col] = pd.to_datetime(df_meta[col], format= '%d/%m/%Y %H:%M:%S')\n",
    "df_meta.drop([\"fluff\"], axis=1, inplace=True)\n",
    "df_meta.reset_index(drop=True, inplace=True)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = pd.read_csv(thesis_path+\"/data/weather/WFS_tables_dump/wfs_values_raw.csv\")\n",
    "\n",
    "df_values.rename(columns= {\n",
    "    \"WEATHERFORECASTSCHIPHOL_ID\": \"id\",\n",
    "    \"ROWNUMBER\": \"row\",\n",
    "    \"DATA\": \"data\"\n",
    "}, inplace=True)\n",
    "\n",
    "row_dict = {\n",
    "    1005: \"t\",\n",
    "    1006: \"f_type\", # Forecast type\n",
    "    2001: \"rvr5000_1000\",\n",
    "    2003: \"rvr1500_300\",\n",
    "    2004: \"rvr550_200\",\n",
    "    2005: \"rvr350\",\n",
    "    2011: \"rvrcat\",\n",
    "    3001: \"wind_dir\",\n",
    "    3002: \"wind_dir_std\",\n",
    "    3011: \"wind_spd\",\n",
    "    3012: \"wind_spd_std\",\n",
    "    3021: \"wind_stoten\",\n",
    "    4001: \"temp\",\n",
    "    4011: \"dew\",\n",
    "    5001: \"snow\",\n",
    "    5002: \"snow_heavy\",\n",
    "    5011: \"rain_cool\",\n",
    "    5021: \"cb\",\n",
    "    5031: \"lightning\",\n",
    "    6001: \"rvr5000_2000\"\n",
    "}\n",
    "\n",
    "df_values = df_values[df_values[\"row\"].isin(row_dict)]\n",
    "df_values['row'].replace(row_dict, inplace=True)\n",
    "df_values['data'] = df_values['data'].str.split(' ')\n",
    "\n",
    "df_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited to values after 2017 for speed\n",
    "df_meta = df_meta[df_meta['id'] >= 37882]\n",
    "df_meta.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = []\n",
    "failed = []\n",
    "for i, meta in df_meta.iterrows():\n",
    "    values = df_values[df_values['id']==meta['id']]\n",
    "    values = values[~values['data'].isna()]\n",
    "    if len(values) == 0:\n",
    "        failed.append(\"No data found for weather ID {}\".format(meta['id']))\n",
    "    elif not 't' in values[['row']].values:\n",
    "        failed.append(\"No time data found for weather ID {}\".format(meta['id']))\n",
    "    elif not 'f_type' in values[['row']].values:\n",
    "        failed.append(\"No forecast time found for weather ID {}\".format(meta['id']))\n",
    "    else:\n",
    "        values = pd.DataFrame(np.array(list(values['data'])).transpose(), columns=values['row'])\n",
    "        # values['t_publish'] = pd.to_datetime( \\\n",
    "        # (values['f_type']=='k') * ((meta['t_publ_short'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')) + \\\n",
    "        # (values['f_type']=='l') * ((meta['t_publ_long'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')), unit='s')\n",
    "\n",
    "        # values['t_test'] = values['t'].copy()\n",
    "\n",
    "        values['t'][1:] = (values['t'].astype(int)-values['t'].astype(int).shift())[1:] # Calculate difference with previous hour\n",
    "        values['t'][0] = int(values['t'][0]) - meta['t_start_short'].hour # Calculate hour difference with meta['t_start_short']\n",
    "        values['t'] = meta['t_start_short'] + pd.to_timedelta((values['t'] + 24*(values['t']<0)).cumsum(), unit='h')\n",
    "\n",
    "        # values.sort_values('t', inplace=True) # Probably not necessary\n",
    "\n",
    "        # values['valid_from'] = pd.to_datetime((values['t'].astype('int64') + values['t'].astype('int64').shift()) / 2)\n",
    "        # values['valid_from'].iloc[0] = values['t'].iloc[0] - pd.Timedelta(30, 'm') * int(values['f_type'].iloc[0]=='k') - pd.Timedelta(90, 'm') * int(values['f_type'].iloc[0]=='l')\n",
    "\n",
    "        # values['valid_till'] = pd.to_datetime((values['t'].astype('int64') + values['t'].astype('int64').shift(-1)) / 2)\n",
    "        # values['valid_till'].iloc[-1] = values['t'].iloc[-1] + pd.Timedelta(30, 'm') * int(values['f_type'].iloc[-1]=='k') + pd.Timedelta(90, 'm') * int(values['f_type'].iloc[-1]=='l')\n",
    "\n",
    "\n",
    "        values['forecast_id'] = meta['id']\n",
    "        values['t_start_short'] = meta['t_start_short']\n",
    "        values['t0'] = (values['t'][0])\n",
    "\n",
    "        data_out.append(values)\n",
    "    if i % int(len(df_meta)/100) == 0:\n",
    "        pct = int(i/int(len(df_meta)/100))\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"\\t[%-20s] %d%%\" % ('='*int(pct/5), pct))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "data_out = pd.concat(data_out)\n",
    "\n",
    "for i in failed:\n",
    "    print(i)\n",
    "\n",
    "# if len(data_out[data_out['t_test'].astype(int)!= data_out['t'].dt.hour]):\n",
    "#     raise Exception(\"\"\"Data Inconsistencies for: \n",
    "#     {}\"\"\".format(data_out[data_out['t_test'].astype(int)!= data_out['t'].dt.hour]))\n",
    "\n",
    "if len(data_out[data_out.duplicated(subset=['forecast_id', 't'], keep=False)]):\n",
    "    raise Exception(\"\"\"Primary Key Violation\n",
    "    {}\"\"\".format(data_out[data_out.duplicated(subset=['forecast_id', 't'], keep=False)]))\n",
    "\n",
    "# 237 s\n",
    "# values[['t_publish', 'f_type', 't', 'valid_from', 'valid_till']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out[(data_out['t_start_short']!=data_out['t0'])][['t', 'forecast_id', 't_start_short', 't0']].drop_duplicates(subset=['forecast_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out[(data_out['t_start_short']!=data_out['t0']) & (data_out['t_start_short'].dt.hour==23)][['t', 'forecast_id', 't_start_short', 't0']].drop_duplicates(subset=['forecast_id'], keep='first').sort_values('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out[(data_out['t_start_short'].dt.date!=data_out['t0'].dt.date) & (data_out['t_start_short'].dt.hour!=23)].drop_duplicates(subset=['forecast_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values['t'][0] = int(values['t'][0]) - meta['t_start_short'].hour # Calculate hour difference with meta['t_start_short']\n",
    "\n",
    "values['t'] = meta['t_start_short'] + pd.to_timedelta((values['t'] + 24*(values['t']<0)).cumsum(), unit='h')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
