{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "durable-climb",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extra-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-remark",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elegant-guinea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['processed_dep_h0', 'processed_dep_h120', 'processed_dep_h180', 'processed_dep_h30', 'raw_dep_h0', 'raw_dep_h120', 'raw_dep_h180', 'raw_dep_h30'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {}\n",
    "for path in glob.glob(r\"../../Data/t_taxi/*.csv\"):\n",
    "    data[path.split('\\\\')[-1].split('.')[0]] = pd.read_csv(path)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frozen-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../Data/t_taxi/csv_docs.json\", \"r\") as f:\n",
    "    docs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-underwear",
   "metadata": {},
   "source": [
    "# Select and Slice Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assumed-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 30\n",
    "df = data['processed_dep_h{}'.format(h)]\n",
    "\n",
    "X_cols_circ = []\n",
    "X_cols_num = []\n",
    "X_cols_cat = []\n",
    "astype_dict = {}\n",
    "for col in df.columns:\n",
    "    if \"dtype\" == col:\n",
    "        continue\n",
    "    if 'circular' in docs[col]['type']:\n",
    "        X_cols_circ.append(col)\n",
    "        docs[col]['type'].append(\"cat\") # just to be sure\n",
    "    if \"num\" in docs[col]['type']:\n",
    "        X_cols_num.append(col)\n",
    "        astype_dict[col] = np.float64\n",
    "    if \"cat\" in docs[col]['type']:\n",
    "        X_cols_cat.append(col)\n",
    "        astype_dict[col] = \"category\"\n",
    "        \n",
    "\n",
    "X_cols = X_cols_cat + X_cols_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coordinate-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[df['dtype']==\"TRAIN\"]\n",
    "X_train.pop(\"dtype\")\n",
    "y_train = X_train.pop(\"t_taxi\")\n",
    "\n",
    "X_val = df[df['dtype']==\"VALIDATE\"]\n",
    "X_val.pop(\"dtype\")\n",
    "y_val = X_val.pop(\"t_taxi\")\n",
    "\n",
    "X_test = df[df['dtype']==\"TEST\"]\n",
    "X_test.pop(\"dtype\")\n",
    "y_test = X_test.pop(\"t_taxi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-metabolism",
   "metadata": {},
   "source": [
    "# Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equivalent-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "def model_eval(y, y_pred, name=None, file=None, verbose=True, **kwargs):\n",
    "    report = {}\n",
    "    if name:\n",
    "        report['name'] = name\n",
    "        if verbose:\n",
    "            print(name)\n",
    "    \n",
    "    report[\"RMSE\"] = mean_squared_error(y, y_pred, squared=False)\n",
    "    report[\"MAE\"] = mean_absolute_error(y, y_pred)\n",
    "    report[\"% <2 min\"] = sum(abs(y-y_pred) < 2*60)/len(y)*100\n",
    "    report[\"% <5 min\"] = sum(abs(y-y_pred) < 5*60)/len(y)*100\n",
    "    report[\"% <7 min\"] = sum(abs(y-y_pred) < 7*60)/len(y)*100\n",
    "    report[\"time\"] = str(pd.Timestamp(round(time.time()), unit='s'))\n",
    "    \n",
    "    for kwarg in kwargs:\n",
    "        report[kwarg] = kwargs[kwarg]\n",
    "    \n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as f:\n",
    "            f.write(str(report)+\"\\n\")\n",
    "    if verbose:\n",
    "        print(report)\n",
    "    return(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-rebound",
   "metadata": {},
   "source": [
    "# Model Preprocessor Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "general-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_preprocessor(X_columns):\n",
    "    ColumnTransformations = []\n",
    "    num_cols = []\n",
    "    cat_cols = []\n",
    "    \n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "    for col in X_columns:\n",
    "        if 'circular' in docs[col]['type']:\n",
    "            ColumnTransformations.append(\n",
    "            (\n",
    "                col + '_qcut_' + str(docs[col]['n_bins']),\n",
    "                QuantileTransformer(n_quantiles=docs[col]['n_bins']),\n",
    "                [col]\n",
    "            ))\n",
    "        if 'num' in docs[col]['type']:\n",
    "            num_cols.append(col)\n",
    "        if 'cat' in docs[col]['type']:\n",
    "            cat_cols.append(col)\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    num_trans = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    ColumnTransformations.append((\"num_trans\", num_trans, num_cols))\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    ColumnTransformations.append((\"onehotencode\", OneHotEncoder(handle_unknown='ignore'), cat_cols))\n",
    "\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    return ColumnTransformer(ColumnTransformations, remainder='drop', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-border",
   "metadata": {},
   "source": [
    "# HistGradientBoostingRegressor, LAD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "experienced-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "likely-creature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 3.828 GB of training data: 25.781 s\n",
      "Binning 0.425 GB of validation data: 0.675 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 31 leaves, max depth = 14, train loss: 203.13133, val loss: 204.44242, in 1.007s\n",
      "[2/100] 1 tree, 31 leaves, max depth = 12, train loss: 191.37860, val loss: 192.67349, in 1.089s\n",
      "[3/100] 1 tree, 31 leaves, max depth = 15, train loss: 181.29348, val loss: 182.59619, in 1.085s\n",
      "[4/100] 1 tree, 31 leaves, max depth = 11, train loss: 172.40038, val loss: 173.67861, in 1.071s\n",
      "[5/100] 1 tree, 31 leaves, max depth = 16, train loss: 164.66822, val loss: 165.94321, in 1.041s\n",
      "[6/100] 1 tree, 31 leaves, max depth = 17, train loss: 158.15373, val loss: 159.43005, in 1.097s\n",
      "[7/100] 1 tree, 31 leaves, max depth = 11, train loss: 152.60178, val loss: 153.90250, in 1.074s\n",
      "[8/100] 1 tree, 31 leaves, max depth = 11, train loss: 147.83821, val loss: 149.14866, in 1.103s\n",
      "[9/100] 1 tree, 31 leaves, max depth = 18, train loss: 143.60003, val loss: 144.91619, in 1.113s\n",
      "[10/100] 1 tree, 31 leaves, max depth = 18, train loss: 139.96057, val loss: 141.28138, in 1.150s\n",
      "[11/100] 1 tree, 31 leaves, max depth = 17, train loss: 136.89902, val loss: 138.18993, in 1.044s\n",
      "[12/100] 1 tree, 31 leaves, max depth = 13, train loss: 134.35657, val loss: 135.62505, in 1.112s\n",
      "[13/100] 1 tree, 31 leaves, max depth = 14, train loss: 132.10897, val loss: 133.38855, in 1.069s\n",
      "[14/100] 1 tree, 31 leaves, max depth = 10, train loss: 130.24263, val loss: 131.44373, in 1.228s\n",
      "[15/100] 1 tree, 31 leaves, max depth = 15, train loss: 128.62246, val loss: 129.81015, in 1.060s\n",
      "[16/100] 1 tree, 31 leaves, max depth = 17, train loss: 127.22791, val loss: 128.34816, in 1.033s\n",
      "[17/100] 1 tree, 31 leaves, max depth = 15, train loss: 125.99455, val loss: 127.09671, in 1.009s\n",
      "[18/100] 1 tree, 31 leaves, max depth = 18, train loss: 124.89950, val loss: 125.96524, in 0.987s\n",
      "[19/100] 1 tree, 31 leaves, max depth = 12, train loss: 123.97014, val loss: 125.04601, in 1.072s\n",
      "[20/100] 1 tree, 31 leaves, max depth = 16, train loss: 123.19271, val loss: 124.26834, in 1.107s\n",
      "[21/100] 1 tree, 31 leaves, max depth = 19, train loss: 122.33761, val loss: 123.38211, in 1.085s\n",
      "[22/100] 1 tree, 31 leaves, max depth = 20, train loss: 121.57816, val loss: 122.61255, in 1.033s\n",
      "[23/100] 1 tree, 31 leaves, max depth = 19, train loss: 120.90563, val loss: 121.91988, in 1.116s\n",
      "[24/100] 1 tree, 31 leaves, max depth = 16, train loss: 120.36115, val loss: 121.41108, in 0.860s\n",
      "[25/100] 1 tree, 31 leaves, max depth = 13, train loss: 119.87944, val loss: 120.96768, in 1.143s\n",
      "[26/100] 1 tree, 31 leaves, max depth = 20, train loss: 119.33706, val loss: 120.41464, in 1.064s\n",
      "[27/100] 1 tree, 31 leaves, max depth = 11, train loss: 118.95322, val loss: 120.02873, in 1.217s\n",
      "[28/100] 1 tree, 31 leaves, max depth = 21, train loss: 118.54157, val loss: 119.59795, in 0.975s\n",
      "[29/100] 1 tree, 31 leaves, max depth = 11, train loss: 118.19278, val loss: 119.25378, in 1.157s\n",
      "[30/100] 1 tree, 31 leaves, max depth = 16, train loss: 117.81263, val loss: 118.86161, in 0.876s\n",
      "[31/100] 1 tree, 31 leaves, max depth = 16, train loss: 117.51895, val loss: 118.56679, in 1.071s\n",
      "[32/100] 1 tree, 31 leaves, max depth = 17, train loss: 117.23745, val loss: 118.28523, in 1.065s\n",
      "[33/100] 1 tree, 31 leaves, max depth = 25, train loss: 116.91769, val loss: 117.96096, in 0.966s\n",
      "[34/100] 1 tree, 31 leaves, max depth = 20, train loss: 116.61286, val loss: 117.65850, in 1.002s\n",
      "[35/100] 1 tree, 31 leaves, max depth = 14, train loss: 116.34898, val loss: 117.38512, in 1.105s\n",
      "[36/100] 1 tree, 31 leaves, max depth = 18, train loss: 116.11528, val loss: 117.17549, in 1.061s\n",
      "[37/100] 1 tree, 31 leaves, max depth = 26, train loss: 115.84949, val loss: 116.89595, in 0.826s\n",
      "[38/100] 1 tree, 31 leaves, max depth = 13, train loss: 115.58622, val loss: 116.65634, in 1.179s\n",
      "[39/100] 1 tree, 31 leaves, max depth = 22, train loss: 115.36877, val loss: 116.43886, in 0.994s\n",
      "[40/100] 1 tree, 31 leaves, max depth = 21, train loss: 115.19503, val loss: 116.27769, in 0.990s\n",
      "[41/100] 1 tree, 31 leaves, max depth = 23, train loss: 114.98443, val loss: 116.07397, in 1.016s\n",
      "[42/100] 1 tree, 31 leaves, max depth = 29, train loss: 114.78398, val loss: 115.84675, in 0.936s\n",
      "[43/100] 1 tree, 31 leaves, max depth = 11, train loss: 114.58892, val loss: 115.66364, in 1.157s\n",
      "[44/100] 1 tree, 31 leaves, max depth = 15, train loss: 114.41858, val loss: 115.51745, in 1.210s\n",
      "[45/100] 1 tree, 31 leaves, max depth = 18, train loss: 114.27140, val loss: 115.37042, in 0.966s\n",
      "[46/100] 1 tree, 31 leaves, max depth = 24, train loss: 114.11821, val loss: 115.22153, in 1.005s\n",
      "[47/100] 1 tree, 31 leaves, max depth = 26, train loss: 113.95955, val loss: 115.07399, in 1.013s\n",
      "[48/100] 1 tree, 31 leaves, max depth = 25, train loss: 113.82348, val loss: 114.94515, in 0.915s\n",
      "[49/100] 1 tree, 31 leaves, max depth = 24, train loss: 113.61801, val loss: 114.74462, in 0.815s\n",
      "[50/100] 1 tree, 31 leaves, max depth = 11, train loss: 113.51219, val loss: 114.64312, in 1.167s\n",
      "[51/100] 1 tree, 31 leaves, max depth = 15, train loss: 113.37838, val loss: 114.52270, in 1.035s\n",
      "[52/100] 1 tree, 31 leaves, max depth = 16, train loss: 113.26256, val loss: 114.40426, in 0.942s\n",
      "[53/100] 1 tree, 31 leaves, max depth = 21, train loss: 113.09063, val loss: 114.24140, in 0.871s\n",
      "[54/100] 1 tree, 31 leaves, max depth = 22, train loss: 112.98336, val loss: 114.13339, in 0.991s\n",
      "[55/100] 1 tree, 31 leaves, max depth = 22, train loss: 112.88222, val loss: 114.04534, in 0.930s\n",
      "[56/100] 1 tree, 31 leaves, max depth = 19, train loss: 112.77397, val loss: 113.93243, in 0.914s\n",
      "[57/100] 1 tree, 31 leaves, max depth = 13, train loss: 112.64279, val loss: 113.79920, in 1.250s\n",
      "[58/100] 1 tree, 31 leaves, max depth = 20, train loss: 112.54974, val loss: 113.70251, in 0.926s\n",
      "[59/100] 1 tree, 31 leaves, max depth = 13, train loss: 112.46852, val loss: 113.62884, in 0.992s\n",
      "[60/100] 1 tree, 31 leaves, max depth = 18, train loss: 112.38335, val loss: 113.55304, in 0.881s\n",
      "[61/100] 1 tree, 31 leaves, max depth = 24, train loss: 112.29872, val loss: 113.46493, in 0.953s\n",
      "[62/100] 1 tree, 31 leaves, max depth = 19, train loss: 112.20727, val loss: 113.37205, in 0.776s\n",
      "[63/100] 1 tree, 31 leaves, max depth = 19, train loss: 112.13616, val loss: 113.29548, in 0.912s\n",
      "[64/100] 1 tree, 31 leaves, max depth = 20, train loss: 112.04718, val loss: 113.21765, in 1.044s\n",
      "[65/100] 1 tree, 31 leaves, max depth = 20, train loss: 111.97555, val loss: 113.14674, in 0.773s\n",
      "[66/100] 1 tree, 31 leaves, max depth = 14, train loss: 111.90784, val loss: 113.07613, in 0.912s\n",
      "[67/100] 1 tree, 31 leaves, max depth = 16, train loss: 111.81324, val loss: 112.99038, in 1.058s\n",
      "[68/100] 1 tree, 31 leaves, max depth = 19, train loss: 111.74982, val loss: 112.93620, in 1.039s\n",
      "[69/100] 1 tree, 31 leaves, max depth = 17, train loss: 111.67591, val loss: 112.86481, in 0.834s\n",
      "[70/100] 1 tree, 31 leaves, max depth = 15, train loss: 111.59832, val loss: 112.80070, in 0.913s\n",
      "[71/100] 1 tree, 31 leaves, max depth = 20, train loss: 111.53253, val loss: 112.74047, in 0.804s\n",
      "[72/100] 1 tree, 31 leaves, max depth = 9, train loss: 111.44764, val loss: 112.67032, in 1.056s\n",
      "[73/100] 1 tree, 31 leaves, max depth = 18, train loss: 111.33889, val loss: 112.56199, in 0.851s\n",
      "[74/100] 1 tree, 31 leaves, max depth = 18, train loss: 111.28101, val loss: 112.52043, in 0.880s\n",
      "[75/100] 1 tree, 31 leaves, max depth = 16, train loss: 111.21547, val loss: 112.44613, in 0.827s\n",
      "[76/100] 1 tree, 31 leaves, max depth = 19, train loss: 111.16181, val loss: 112.38887, in 0.764s\n",
      "[77/100] 1 tree, 31 leaves, max depth = 18, train loss: 111.09894, val loss: 112.32789, in 0.783s\n",
      "[78/100] 1 tree, 31 leaves, max depth = 18, train loss: 111.04593, val loss: 112.27413, in 0.901s\n",
      "[79/100] 1 tree, 31 leaves, max depth = 13, train loss: 110.94950, val loss: 112.18511, in 1.127s\n",
      "[80/100] 1 tree, 31 leaves, max depth = 19, train loss: 110.90260, val loss: 112.13786, in 0.806s\n",
      "[81/100] 1 tree, 31 leaves, max depth = 12, train loss: 110.83697, val loss: 112.09833, in 0.870s\n",
      "[82/100] 1 tree, 31 leaves, max depth = 17, train loss: 110.77202, val loss: 112.04615, in 0.822s\n",
      "[83/100] 1 tree, 31 leaves, max depth = 10, train loss: 110.72595, val loss: 112.00751, in 0.987s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84/100] 1 tree, 31 leaves, max depth = 16, train loss: 110.66527, val loss: 111.94276, in 0.986s\n",
      "[85/100] 1 tree, 31 leaves, max depth = 13, train loss: 110.60508, val loss: 111.89252, in 0.807s\n",
      "[86/100] 1 tree, 31 leaves, max depth = 17, train loss: 110.54198, val loss: 111.83817, in 0.925s\n",
      "[87/100] 1 tree, 31 leaves, max depth = 15, train loss: 110.46619, val loss: 111.77274, in 1.024s\n",
      "[88/100] 1 tree, 31 leaves, max depth = 19, train loss: 110.40132, val loss: 111.70914, in 0.764s\n",
      "[89/100] 1 tree, 31 leaves, max depth = 18, train loss: 110.35606, val loss: 111.66737, in 0.785s\n",
      "[90/100] 1 tree, 31 leaves, max depth = 16, train loss: 110.30739, val loss: 111.63101, in 0.734s\n",
      "[91/100] 1 tree, 31 leaves, max depth = 14, train loss: 110.23847, val loss: 111.57488, in 1.001s\n",
      "[92/100] 1 tree, 31 leaves, max depth = 11, train loss: 110.18858, val loss: 111.52818, in 0.872s\n",
      "[93/100] 1 tree, 31 leaves, max depth = 14, train loss: 110.14180, val loss: 111.49136, in 0.902s\n",
      "[94/100] 1 tree, 31 leaves, max depth = 13, train loss: 110.10150, val loss: 111.45596, in 0.733s\n",
      "[95/100] 1 tree, 31 leaves, max depth = 17, train loss: 110.05153, val loss: 111.41111, in 0.782s\n",
      "[96/100] 1 tree, 31 leaves, max depth = 14, train loss: 109.99005, val loss: 111.35230, in 0.990s\n",
      "[97/100] 1 tree, 31 leaves, max depth = 15, train loss: 109.94669, val loss: 111.31118, in 0.796s\n",
      "[98/100] 1 tree, 31 leaves, max depth = 11, train loss: 109.90579, val loss: 111.28035, in 0.773s\n",
      "[99/100] 1 tree, 31 leaves, max depth = 17, train loss: 109.86935, val loss: 111.25042, in 0.719s\n",
      "[100/100] 1 tree, 31 leaves, max depth = 13, train loss: 109.81998, val loss: 111.21669, in 0.724s\n",
      "Fit 100 trees in 125.697 s, (3100 total leaves)\n",
      "Time spent computing histograms: 58.194s\n",
      "Time spent finding best splits:  0.772s\n",
      "Time spent applying splits:      0.806s\n",
      "Time spent predicting:           0.045s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Preprocessor',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('local_doy_qcut_24',\n",
       "                                                  QuantileTransformer(n_quantiles=24),\n",
       "                                                  ['local_doy']),\n",
       "                                                 ('local_mod_qcut_48',\n",
       "                                                  QuantileTransformer(n_quantiles=48),\n",
       "                                                  ['local_mod']),\n",
       "                                                 ('num_trans',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['n_civil', 'n_dep', 'n_arr',\n",
       "                                                   '...\n",
       "                                                  ['trwy', 'actype', 'depgnr',\n",
       "                                                   'sid', 'wtc', 'rvrcat',\n",
       "                                                   'wind_dir', 'plr1', 'plr2',\n",
       "                                                   'ptr1', 'ptr2', 'alr1',\n",
       "                                                   'alr2', 'atr1', 'atr2',\n",
       "                                                   'local_doy', 'local_dow',\n",
       "                                                   'local_mod', 'local_week',\n",
       "                                                   'trwy_ext'])])),\n",
       "                ('Densifier',\n",
       "                 <__main__.DenseTransformer object at 0x000001F70D0C5280>),\n",
       "                ('Regressor',\n",
       "                 HistGradientBoostingRegressor(loss='least_absolute_deviation',\n",
       "                                               random_state=0, verbose=1))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, \\\n",
    "    BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "regr = HistGradientBoostingRegressor(\n",
    "        loss='least_absolute_deviation',\n",
    "        random_state=0,\n",
    "        verbose=1,\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "    (\"Densifier\", DenseTransformer()),\n",
    "    (\"Regressor\", regr),\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smart-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RMSE': 176.77051446327275, 'MAE': 116.03181408174824, '% <2 min': 65.96125541986555, '% <5 min': 93.17395282230797, '% <7 min': 97.36465253192252, 'time': '2021-04-22 15:48:05'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RMSE': 176.77051446327275,\n",
       " 'MAE': 116.03181408174824,\n",
       " '% <2 min': 65.96125541986555,\n",
       " '% <5 min': 93.17395282230797,\n",
       " '% <7 min': 97.36465253192252,\n",
       " 'time': '2021-04-22 15:48:05'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval(\n",
    "        y_val,\n",
    "        model.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-stockholm",
   "metadata": {},
   "source": [
    "# Tree Based Ensemble Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "signal-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "front-acceptance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4 GradientBoostingRegressor\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       71966.7967            1.26m\n",
      "         2       65440.2055            1.20m\n",
      "         3       60136.1355            1.16m\n",
      "         4       55820.6686            1.13m\n",
      "         5       52280.2802            1.10m\n",
      "         6       49393.4052            1.09m\n",
      "         7       47020.0058            1.07m\n",
      "         8       44970.4887            1.06m\n",
      "         9       43303.9000            1.04m\n",
      "        10       41945.5090            1.03m\n",
      "        20       35522.7104           53.58s\n",
      "        30       33477.3194           46.25s\n",
      "        40       32447.9447           38.95s\n",
      "        50       31752.1928           32.05s\n",
      "        60       31200.5208           25.46s\n",
      "        70       30775.0851           18.99s\n",
      "        80       30392.6615           12.62s\n",
      "        90       30076.5694            6.30s\n",
      "       100       29791.9979            0.00s\n",
      "LR_varsearch_GradientBoostingRegressor_h30\n",
      "{'name': 'LR_varsearch_GradientBoostingRegressor_h30', 'RMSE': 183.19168041332432, 'MAE': 127.09721535325652, '% <2 min': 58.64791757826485, '% <5 min': 93.23362106686821, '% <7 min': 97.31492899478897, 'time': '2021-04-27 09:50:43', 't': 69, 'eval_type': 'SIMPLE_VAL', 'model_type': 'LR_varsearch_GradientBoostingRegressor', 'dataset_train': 'processed_dep_h30', 'dataset_test': 'processed_dep_h30'}\n",
      "1 / 4 GradientBoostingRegressor, LAD\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         204.5490            1.34m\n",
      "         2         193.9913            1.27m\n",
      "         3         185.1503            1.23m\n",
      "         4         177.3586            1.20m\n",
      "         5         170.7810            1.19m\n",
      "         6         165.1586            1.17m\n",
      "         7         160.3421            1.16m\n",
      "         8         156.3167            1.15m\n",
      "         9         152.8746            1.14m\n",
      "        10         149.9128            1.13m\n",
      "        20         134.9064           57.71s\n",
      "        30         129.8270           49.68s\n",
      "        40         127.5093           41.88s\n",
      "        50         125.9171           34.49s\n",
      "        60         124.7269           27.36s\n",
      "        70         123.6701           20.38s\n",
      "        80         122.8542           13.51s\n",
      "        90         122.1608            6.71s\n",
      "       100         121.5670            0.00s\n",
      "LR_varsearch_GradientBoostingRegressor, LAD_h30\n",
      "{'name': 'LR_varsearch_GradientBoostingRegressor, LAD_h30', 'RMSE': 185.25021551765025, 'MAE': 125.1113677361511, '% <2 min': 61.720832173117465, '% <5 min': 92.43406658976093, '% <7 min': 97.0046541230757, 'time': '2021-04-27 09:51:55', 't': 72, 'eval_type': 'SIMPLE_VAL', 'model_type': 'LR_varsearch_GradientBoostingRegressor, LAD', 'dataset_train': 'processed_dep_h30', 'dataset_test': 'processed_dep_h30'}\n",
      "2 / 4 GradientBoostingRegressor, Huber\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       33814.9760            1.30m\n",
      "         2       30377.9444            1.24m\n",
      "         3       27613.5102            1.22m\n",
      "         4       25369.4691            1.19m\n",
      "         5       23536.1398            1.17m\n",
      "         6       22027.5457            1.16m\n",
      "         7       20798.8873            1.14m\n",
      "         8       19751.6185            1.13m\n",
      "         9       18884.6318            1.11m\n",
      "        10       18127.9275            1.10m\n",
      "        20       14752.4912           58.02s\n",
      "        30       13741.6359           50.40s\n",
      "        40       13245.8952           42.66s\n",
      "        50       12916.3925           35.36s\n",
      "        60       12678.8901           28.22s\n",
      "        70       12489.9391           21.17s\n",
      "        80       12338.3461           14.07s\n",
      "        90       12207.2903            7.03s\n",
      "       100       12093.1889            0.00s\n",
      "LR_varsearch_GradientBoostingRegressor, Huber_h30\n",
      "{'name': 'LR_varsearch_GradientBoostingRegressor, Huber_h30', 'RMSE': 183.76370165125363, 'MAE': 126.0234697057699, '% <2 min': 60.048132383945266, '% <5 min': 92.92931301961096, '% <7 min': 97.15979155893234, 'time': '2021-04-27 09:53:10', 't': 75, 'eval_type': 'SIMPLE_VAL', 'model_type': 'LR_varsearch_GradientBoostingRegressor, Huber', 'dataset_train': 'processed_dep_h30', 'dataset_test': 'processed_dep_h30'}\n",
      "3 / 4 GradientBoostingRegressor, Quantile\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1          54.6182            1.30m\n",
      "         2          51.7289            1.22m\n",
      "         3          49.3053            1.19m\n",
      "         4          47.2718            1.17m\n",
      "         5          45.5752            1.16m\n",
      "         6          44.1427            1.14m\n",
      "         7          42.9416            1.13m\n",
      "         8          41.9273            1.12m\n",
      "         9          41.0679            1.10m\n",
      "        10          40.3600            1.09m\n",
      "        20          36.6936           56.55s\n",
      "        30          35.4060           48.44s\n",
      "        40          34.6499           41.42s\n",
      "        50          34.2250           34.29s\n",
      "        60          33.8759           27.35s\n",
      "        70          33.6310           20.48s\n",
      "        80          33.4316           13.56s\n",
      "        90          33.2253            6.75s\n",
      "       100          33.0517            0.00s\n",
      "LR_varsearch_GradientBoostingRegressor, Quantile_h30\n",
      "{'name': 'LR_varsearch_GradientBoostingRegressor, Quantile_h30', 'RMSE': 262.4214886716039, 'MAE': 220.50719232673222, '% <2 min': 24.105970802338994, '% <5 min': 75.23370062452763, '% <7 min': 94.21814710211225, 'time': '2021-04-27 09:54:22', 't': 72, 'eval_type': 'SIMPLE_VAL', 'model_type': 'LR_varsearch_GradientBoostingRegressor, Quantile', 'dataset_train': 'processed_dep_h30', 'dataset_test': 'processed_dep_h30'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, \\\n",
    "    BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "regrs = {\n",
    "#     \"BaggingRegressor\": BaggingRegressor(\n",
    "#         random_state=0,\n",
    "#         verbose=1,\n",
    "#         n_jobs=8,\n",
    "#     ),\n",
    "#     \"ExtraTreesRegressor\": ExtraTreesRegressor( # Criterion MAE?\n",
    "#         random_state=0,\n",
    "#         verbose=1,\n",
    "#         n_jobs=8,\n",
    "#     ),\n",
    "#     \"AdaBoostRegressor\": AdaBoostRegressor(\n",
    "#         random_state=0,\n",
    "#     ),\n",
    "#     \"RandomForestRegressor\": RandomForestRegressor( # Criterion MAE\n",
    "#         random_state=0,\n",
    "#         verbose=1,\n",
    "#         n_jobs=8,\n",
    "#     ),\n",
    "#     \"HistGradientBoostingRegressor\": HistGradientBoostingRegressor(\n",
    "#         random_state=0,\n",
    "#         verbose=1,\n",
    "#     ),\n",
    "#     \"HistGradientBoostingRegressorLAD\": HistGradientBoostingRegressor(\n",
    "#         loss='least_absolute_deviation',\n",
    "#         random_state=0,\n",
    "#         verbose=1,\n",
    "#     ),\n",
    "#     \"RandomForestRegressorMAE\": RandomForestRegressor( # Criterion MAE\n",
    "#         criterion='mae',\n",
    "#         random_state=0,\n",
    "#         verbose=1,\n",
    "#         n_jobs=8,\n",
    "#     ),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(\n",
    "        verbose=1,\n",
    "    ),\n",
    "    \"GradientBoostingRegressor, LAD\": GradientBoostingRegressor(\n",
    "        loss= 'lad',\n",
    "        verbose=1,\n",
    "    ),\n",
    "    \"GradientBoostingRegressor, Huber\": GradientBoostingRegressor(\n",
    "        loss= 'huber',\n",
    "        verbose=1,\n",
    "    ),\n",
    "    \"GradientBoostingRegressor, Quantile\": GradientBoostingRegressor(\n",
    "        loss= 'quantile',\n",
    "        verbose=1,\n",
    "    ),\n",
    "}\n",
    "\n",
    "for i, regr in enumerate(regrs):\n",
    "    print(i, '/', len(regrs), str(regr).split('(')[0])\n",
    "    t = time.time()\n",
    "    \n",
    "    model = Pipeline(steps=[\n",
    "        (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "        (\"Regressor\", regrs[regr])\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "    except Exception as inst:\n",
    "        print(\"Error\")\n",
    "        print(type(inst))\n",
    "        print(inst)\n",
    "        try:\n",
    "            model = Pipeline(steps=[\n",
    "                (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "                (\"Densifier\", DenseTransformer()),\n",
    "                (\"Regressor\", regr),\n",
    "            ])\n",
    "            model.fit(X_train, y_train)\n",
    "            print(\"Fix Worked\")\n",
    "        except Exception as inst:\n",
    "            print(\"Fix Did Not Work\")\n",
    "            print(type(inst))\n",
    "            print(inst)\n",
    "            continue\n",
    "    \n",
    "    results.append(model_eval(\n",
    "        y_val,\n",
    "        model.predict(X_val),\n",
    "        t = round(time.time()-t),\n",
    "        name = \"LR_varsearch_{}_h30\".format(regr),\n",
    "        eval_type = \"SIMPLE_VAL\",\n",
    "        model_type = \"LR_varsearch_{}\".format(regr),\n",
    "        dataset_train = \"processed_dep_h30\",\n",
    "        dataset_test = \"processed_dep_h30\",\n",
    "    ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-donna",
   "metadata": {},
   "source": [
    "RandomForestRegressor MAE: Does not converge within eight hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-pledge",
   "metadata": {},
   "source": [
    "# Final Model Evaluation Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sacred-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 3.828 GB of training data: 25.181 s\n",
      "Binning 0.425 GB of validation data: 0.661 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 1 tree, 31 leaves, max depth = 14, train loss: 203.13133, val loss: 204.44242, in 0.941s\n",
      "[2/100] 1 tree, 31 leaves, max depth = 12, train loss: 191.37860, val loss: 192.67349, in 1.111s\n",
      "[3/100] 1 tree, 31 leaves, max depth = 15, train loss: 181.29348, val loss: 182.59619, in 1.096s\n",
      "[4/100] 1 tree, 31 leaves, max depth = 11, train loss: 172.40038, val loss: 173.67861, in 1.056s\n",
      "[5/100] 1 tree, 31 leaves, max depth = 16, train loss: 164.66822, val loss: 165.94321, in 1.030s\n",
      "[6/100] 1 tree, 31 leaves, max depth = 17, train loss: 158.15373, val loss: 159.43005, in 1.167s\n",
      "[7/100] 1 tree, 31 leaves, max depth = 11, train loss: 152.60178, val loss: 153.90250, in 1.115s\n",
      "[8/100] 1 tree, 31 leaves, max depth = 11, train loss: 147.83821, val loss: 149.14866, in 1.156s\n",
      "[9/100] 1 tree, 31 leaves, max depth = 18, train loss: 143.60003, val loss: 144.91619, in 1.061s\n",
      "[10/100] 1 tree, 31 leaves, max depth = 18, train loss: 139.96057, val loss: 141.28138, in 1.134s\n",
      "[11/100] 1 tree, 31 leaves, max depth = 17, train loss: 136.89902, val loss: 138.18993, in 1.107s\n",
      "[12/100] 1 tree, 31 leaves, max depth = 13, train loss: 134.35657, val loss: 135.62505, in 1.145s\n",
      "[13/100] 1 tree, 31 leaves, max depth = 14, train loss: 132.10897, val loss: 133.38855, in 1.003s\n",
      "[14/100] 1 tree, 31 leaves, max depth = 10, train loss: 130.24263, val loss: 131.44373, in 1.053s\n",
      "[15/100] 1 tree, 31 leaves, max depth = 15, train loss: 128.62246, val loss: 129.81015, in 0.857s\n",
      "[16/100] 1 tree, 31 leaves, max depth = 17, train loss: 127.22791, val loss: 128.34816, in 0.939s\n",
      "[17/100] 1 tree, 31 leaves, max depth = 15, train loss: 125.99455, val loss: 127.09671, in 0.918s\n",
      "[18/100] 1 tree, 31 leaves, max depth = 18, train loss: 124.89950, val loss: 125.96524, in 0.828s\n",
      "[19/100] 1 tree, 31 leaves, max depth = 12, train loss: 123.97014, val loss: 125.04601, in 0.916s\n",
      "[20/100] 1 tree, 31 leaves, max depth = 16, train loss: 123.19271, val loss: 124.26834, in 0.917s\n",
      "[21/100] 1 tree, 31 leaves, max depth = 19, train loss: 122.33761, val loss: 123.38211, in 0.923s\n",
      "[22/100] 1 tree, 31 leaves, max depth = 20, train loss: 121.57816, val loss: 122.61255, in 0.928s\n",
      "[23/100] 1 tree, 31 leaves, max depth = 19, train loss: 120.90563, val loss: 121.91988, in 0.997s\n",
      "[24/100] 1 tree, 31 leaves, max depth = 16, train loss: 120.36115, val loss: 121.41108, in 0.760s\n",
      "[25/100] 1 tree, 31 leaves, max depth = 13, train loss: 119.87944, val loss: 120.96768, in 0.980s\n",
      "[26/100] 1 tree, 31 leaves, max depth = 20, train loss: 119.33706, val loss: 120.41464, in 0.871s\n",
      "[27/100] 1 tree, 31 leaves, max depth = 11, train loss: 118.95322, val loss: 120.02873, in 1.059s\n",
      "[28/100] 1 tree, 31 leaves, max depth = 21, train loss: 118.54157, val loss: 119.59795, in 0.782s\n",
      "[29/100] 1 tree, 31 leaves, max depth = 11, train loss: 118.19278, val loss: 119.25378, in 1.066s\n",
      "[30/100] 1 tree, 31 leaves, max depth = 16, train loss: 117.81263, val loss: 118.86161, in 0.751s\n",
      "[31/100] 1 tree, 31 leaves, max depth = 16, train loss: 117.51895, val loss: 118.56679, in 0.865s\n",
      "[32/100] 1 tree, 31 leaves, max depth = 17, train loss: 117.23745, val loss: 118.28523, in 0.894s\n",
      "[33/100] 1 tree, 31 leaves, max depth = 25, train loss: 116.91769, val loss: 117.96096, in 0.914s\n",
      "[34/100] 1 tree, 31 leaves, max depth = 20, train loss: 116.61286, val loss: 117.65850, in 0.933s\n",
      "[35/100] 1 tree, 31 leaves, max depth = 14, train loss: 116.34898, val loss: 117.38512, in 0.929s\n",
      "[36/100] 1 tree, 31 leaves, max depth = 18, train loss: 116.11528, val loss: 117.17549, in 0.799s\n",
      "[37/100] 1 tree, 31 leaves, max depth = 26, train loss: 115.84949, val loss: 116.89595, in 0.706s\n",
      "[38/100] 1 tree, 31 leaves, max depth = 13, train loss: 115.58622, val loss: 116.65634, in 0.940s\n",
      "[39/100] 1 tree, 31 leaves, max depth = 22, train loss: 115.36877, val loss: 116.43886, in 0.875s\n",
      "[40/100] 1 tree, 31 leaves, max depth = 21, train loss: 115.19503, val loss: 116.27769, in 0.905s\n",
      "[41/100] 1 tree, 31 leaves, max depth = 23, train loss: 114.98443, val loss: 116.07397, in 1.590s\n",
      "[42/100] 1 tree, 31 leaves, max depth = 29, train loss: 114.78398, val loss: 115.84675, in 1.625s\n",
      "[43/100] 1 tree, 31 leaves, max depth = 11, train loss: 114.58892, val loss: 115.66364, in 1.416s\n",
      "[44/100] 1 tree, 31 leaves, max depth = 15, train loss: 114.41858, val loss: 115.51745, in 1.237s\n",
      "[45/100] 1 tree, 31 leaves, max depth = 18, train loss: 114.27140, val loss: 115.37042, in 1.010s\n",
      "[46/100] 1 tree, 31 leaves, max depth = 24, train loss: 114.11821, val loss: 115.22153, in 0.912s\n",
      "[47/100] 1 tree, 31 leaves, max depth = 26, train loss: 113.95955, val loss: 115.07399, in 1.008s\n",
      "[48/100] 1 tree, 31 leaves, max depth = 25, train loss: 113.82348, val loss: 114.94515, in 0.868s\n",
      "[49/100] 1 tree, 31 leaves, max depth = 24, train loss: 113.61801, val loss: 114.74462, in 0.901s\n",
      "[50/100] 1 tree, 31 leaves, max depth = 11, train loss: 113.51219, val loss: 114.64312, in 1.205s\n",
      "[51/100] 1 tree, 31 leaves, max depth = 15, train loss: 113.37838, val loss: 114.52270, in 0.984s\n",
      "[52/100] 1 tree, 31 leaves, max depth = 16, train loss: 113.26256, val loss: 114.40426, in 0.986s\n",
      "[53/100] 1 tree, 31 leaves, max depth = 21, train loss: 113.09063, val loss: 114.24140, in 0.874s\n",
      "[54/100] 1 tree, 31 leaves, max depth = 22, train loss: 112.98336, val loss: 114.13339, in 0.968s\n",
      "[55/100] 1 tree, 31 leaves, max depth = 22, train loss: 112.88222, val loss: 114.04534, in 0.888s\n",
      "[56/100] 1 tree, 31 leaves, max depth = 19, train loss: 112.77397, val loss: 113.93243, in 0.841s\n",
      "[57/100] 1 tree, 31 leaves, max depth = 13, train loss: 112.64279, val loss: 113.79920, in 1.206s\n",
      "[58/100] 1 tree, 31 leaves, max depth = 20, train loss: 112.54974, val loss: 113.70251, in 0.920s\n",
      "[59/100] 1 tree, 31 leaves, max depth = 13, train loss: 112.46852, val loss: 113.62884, in 1.071s\n",
      "[60/100] 1 tree, 31 leaves, max depth = 18, train loss: 112.38335, val loss: 113.55304, in 0.899s\n",
      "[61/100] 1 tree, 31 leaves, max depth = 24, train loss: 112.29872, val loss: 113.46493, in 0.929s\n",
      "[62/100] 1 tree, 31 leaves, max depth = 19, train loss: 112.20727, val loss: 113.37205, in 0.764s\n",
      "[63/100] 1 tree, 31 leaves, max depth = 19, train loss: 112.13616, val loss: 113.29548, in 0.921s\n",
      "[64/100] 1 tree, 31 leaves, max depth = 20, train loss: 112.04718, val loss: 113.21765, in 1.003s\n",
      "[65/100] 1 tree, 31 leaves, max depth = 20, train loss: 111.97555, val loss: 113.14674, in 0.824s\n",
      "[66/100] 1 tree, 31 leaves, max depth = 14, train loss: 111.90784, val loss: 113.07613, in 0.913s\n",
      "[67/100] 1 tree, 31 leaves, max depth = 16, train loss: 111.81324, val loss: 112.99038, in 1.070s\n",
      "[68/100] 1 tree, 31 leaves, max depth = 19, train loss: 111.74982, val loss: 112.93620, in 0.946s\n",
      "[69/100] 1 tree, 31 leaves, max depth = 17, train loss: 111.67591, val loss: 112.86481, in 0.851s\n",
      "[70/100] 1 tree, 31 leaves, max depth = 15, train loss: 111.59832, val loss: 112.80070, in 0.963s\n",
      "[71/100] 1 tree, 31 leaves, max depth = 20, train loss: 111.53253, val loss: 112.74047, in 0.761s\n",
      "[72/100] 1 tree, 31 leaves, max depth = 9, train loss: 111.44764, val loss: 112.67032, in 1.040s\n",
      "[73/100] 1 tree, 31 leaves, max depth = 18, train loss: 111.33889, val loss: 112.56199, in 0.872s\n",
      "[74/100] 1 tree, 31 leaves, max depth = 18, train loss: 111.28101, val loss: 112.52043, in 0.848s\n",
      "[75/100] 1 tree, 31 leaves, max depth = 16, train loss: 111.21547, val loss: 112.44613, in 0.836s\n",
      "[76/100] 1 tree, 31 leaves, max depth = 19, train loss: 111.16181, val loss: 112.38887, in 0.785s\n",
      "[77/100] 1 tree, 31 leaves, max depth = 18, train loss: 111.09894, val loss: 112.32789, in 0.757s\n",
      "[78/100] 1 tree, 31 leaves, max depth = 18, train loss: 111.04593, val loss: 112.27413, in 0.814s\n",
      "[79/100] 1 tree, 31 leaves, max depth = 13, train loss: 110.94950, val loss: 112.18511, in 1.041s\n",
      "[80/100] 1 tree, 31 leaves, max depth = 19, train loss: 110.90260, val loss: 112.13786, in 0.824s\n",
      "[81/100] 1 tree, 31 leaves, max depth = 12, train loss: 110.83697, val loss: 112.09833, in 0.935s\n",
      "[82/100] 1 tree, 31 leaves, max depth = 17, train loss: 110.77202, val loss: 112.04615, in 0.816s\n",
      "[83/100] 1 tree, 31 leaves, max depth = 10, train loss: 110.72595, val loss: 112.00751, in 0.948s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84/100] 1 tree, 31 leaves, max depth = 16, train loss: 110.66527, val loss: 111.94276, in 0.894s\n",
      "[85/100] 1 tree, 31 leaves, max depth = 13, train loss: 110.60508, val loss: 111.89252, in 0.788s\n",
      "[86/100] 1 tree, 31 leaves, max depth = 17, train loss: 110.54198, val loss: 111.83817, in 0.927s\n",
      "[87/100] 1 tree, 31 leaves, max depth = 15, train loss: 110.46619, val loss: 111.77274, in 1.064s\n",
      "[88/100] 1 tree, 31 leaves, max depth = 19, train loss: 110.40132, val loss: 111.70914, in 0.746s\n",
      "[89/100] 1 tree, 31 leaves, max depth = 18, train loss: 110.35606, val loss: 111.66737, in 0.724s\n",
      "[90/100] 1 tree, 31 leaves, max depth = 16, train loss: 110.30739, val loss: 111.63101, in 0.698s\n",
      "[91/100] 1 tree, 31 leaves, max depth = 14, train loss: 110.23847, val loss: 111.57488, in 1.017s\n",
      "[92/100] 1 tree, 31 leaves, max depth = 11, train loss: 110.18858, val loss: 111.52818, in 0.882s\n",
      "[93/100] 1 tree, 31 leaves, max depth = 14, train loss: 110.14180, val loss: 111.49136, in 0.975s\n",
      "[94/100] 1 tree, 31 leaves, max depth = 13, train loss: 110.10150, val loss: 111.45596, in 0.755s\n",
      "[95/100] 1 tree, 31 leaves, max depth = 17, train loss: 110.05153, val loss: 111.41111, in 0.756s\n",
      "[96/100] 1 tree, 31 leaves, max depth = 14, train loss: 109.99005, val loss: 111.35230, in 1.104s\n",
      "[97/100] 1 tree, 31 leaves, max depth = 15, train loss: 109.94669, val loss: 111.31118, in 0.796s\n",
      "[98/100] 1 tree, 31 leaves, max depth = 11, train loss: 109.90579, val loss: 111.28035, in 0.803s\n",
      "[99/100] 1 tree, 31 leaves, max depth = 17, train loss: 109.86935, val loss: 111.25042, in 0.752s\n",
      "[100/100] 1 tree, 31 leaves, max depth = 13, train loss: 109.81998, val loss: 111.21669, in 0.714s\n",
      "Fit 100 trees in 122.345 s, (3100 total leaves)\n",
      "Time spent computing histograms: 55.856s\n",
      "Time spent finding best splits:  0.924s\n",
      "Time spent applying splits:      0.972s\n",
      "Time spent predicting:           0.046s\n",
      "LR_varsearch_HistGradientBoostingRegressor(loss='least_absolute_deviation', random_state=0,\n",
      "                              verbose=1)_h30\n",
      "{'name': \"LR_varsearch_HistGradientBoostingRegressor(loss='least_absolute_deviation', random_state=0,\\n                              verbose=1)_h30\", 'RMSE': 176.77051446327275, 'MAE': 116.03181408174824, '% <2 min': 65.96125541986555, '% <5 min': 93.17395282230797, '% <7 min': 97.36465253192252, 'time': '2021-04-23 08:31:27', 't': 549, 'eval_type': 'SIMPLE_VAL', 'model_type': \"LR_varsearch_HistGradientBoostingRegressor(loss='least_absolute_deviation', random_state=0,\\n                              verbose=1)\", 'dataset_train': 'processed_dep_h30', 'dataset_test': 'processed_dep_h30'}\n"
     ]
    }
   ],
   "source": [
    "regr_name = \"HistGradientBoostingRegressor, loss='least_absolute_deviation'\"\n",
    "regr = HistGradientBoostingRegressor(\n",
    "        random_state=0,\n",
    "        verbose=1,\n",
    "        loss='least_absolute_deviation',\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "    (\"Densifier\", DenseTransformer()),\n",
    "    (\"Regressor\", regr),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "results.append(model_eval(\n",
    "    y_val,\n",
    "    model.predict(X_val),\n",
    "    t = round(time.time()-t),\n",
    "    name = \"LR_varsearch_{}_h30\".format(regr),\n",
    "    eval_type = \"SIMPLE_VAL\",\n",
    "    model_type = \"LR_varsearch_{}\".format(regr),\n",
    "    dataset_train = \"processed_dep_h30\",\n",
    "    dataset_test = \"processed_dep_h30\",\n",
    ")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-flashing",
   "metadata": {},
   "source": [
    "# Save/Load Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "employed-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\"random_forest_variations_results.csv\"),\n",
    "        pd.DataFrame(results)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adverse-transcript",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"random_forest_variations_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "willing-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"random_forest_variations_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-relay",
   "metadata": {},
   "source": [
    "# Figure: tbe Model Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "subsequent-wrestling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BaggingRegressor', 'ExtraTreesRegressor', 'AdaBoostRegressor', 'RandomForestRegressor', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor, LAD', 'GradientBoostingRegressor', 'GradientBoostingRegressor, LAD', 'GradientBoostingRegressor, Huber', 'GradientBoostingRegressor, Quantile']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "out = results\n",
    "# out = out[out['t'].astype(np.float64) < 1000]\n",
    "X = out['t'].astype(np.float64)\n",
    "Y = out['% <2 min']\n",
    "C = cm.rainbow(np.linspace(0, 1, len(out)))\n",
    "labels = out['Variation'].to_list()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    labels[i] = label.replace(\"\\_\", \", \")\n",
    "\n",
    "print(labels)\n",
    "\n",
    "markers = [\n",
    "    \".\", \",\", \"o\", \"v\", \"^\" , \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \n",
    "    \"s\", \"p\", \"P\", \"*\" , \"h\", \"H\", \"+\", \"x\", \"X\", \"D\", \"d\", \"|\",\n",
    "    \"_\", 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,]\n",
    "markers = markers[:len(X)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for (x, y, c, label, marker) in zip(X, Y, C, labels, markers):\n",
    "    ax.scatter(x, y, color=c, label=label, marker=marker)\n",
    "    \n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='Times New Roman')\n",
    "\n",
    "\n",
    "ax.legend(loc='lower right',)\n",
    "ax.grid(True)\n",
    "plt.xlabel(\"t [s]\")\n",
    "plt.ylabel(\"$|\\epsilon| < 2$ min [\\%]\")\n",
    "fig.savefig(\"rf_var_plot.png\")\n",
    "fig.savefig('rf_var_plot.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-liberty",
   "metadata": {},
   "source": [
    "# Table: tbe model variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "expanded-commons",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "                         Variation &  \\% <2 min &  MAE [s] &  RMSE [s] &  \\% <5 min &  \\% <7 min &  Computation Time [s] \\\\\n",
      "\\midrule\n",
      "                  BaggingRegressor &     63.58 &   120.44 &    181.32 &     92.98 &     97.23 &                1159.0 \\\\\n",
      "               ExtraTreesRegressor &     64.19 &   120.60 &    184.64 &     92.55 &     97.01 &               14593.0 \\\\\n",
      "                 AdaBoostRegressor &      9.48 &   419.73 &    476.10 &     29.25 &     49.39 &                  37.0 \\\\\n",
      "             RandomForestRegressor &     65.40 &   116.12 &    176.15 &     93.40 &     97.49 &                9049.0 \\\\\n",
      "     HistGradientBoostingRegressor &     63.84 &   117.36 &    173.90 &     93.91 &     97.73 &                 284.0 \\\\\n",
      "     HistGradientBoostingRegressor &     65.96 &   116.03 &    176.77 &     93.17 &     97.36 &                 549.0 \\\\\n",
      "         GradientBoostingRegressor &     58.65 &   127.10 &    183.19 &     93.23 &     97.31 &                  69.0 \\\\\n",
      "     GradientBoostingRegressor\\_LAD &     61.72 &   125.11 &    185.25 &     92.43 &     97.00 &                  72.0 \\\\\n",
      "   GradientBoostingRegressor\\_Huber &     60.05 &   126.02 &    183.76 &     92.93 &     97.16 &                  75.0 \\\\\n",
      "GradientBoostingRegressor\\_Quantile &     24.11 &   220.51 &    262.42 &     75.23 &     94.22 &                  72.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = pd.DataFrame(results)\n",
    "out['label'] = out['name'].str.extract(\"LR_varsearch_(.*)_h30.*\").values[:,0]\n",
    "print(\n",
    "out[\n",
    "    [\n",
    "        'label', \n",
    "        '% <2 min', \n",
    "        'MAE', \n",
    "        \"RMSE\", \n",
    "        '% <5 min', \n",
    "        '% <7 min', \n",
    "        't'\n",
    "    ]\n",
    "].rename(columns=\n",
    "         {\"label\": \"Variation\", \n",
    "          \"MAE\": \"MAE [s]\",\n",
    "          \"RMSE\": \"RMSE [s]\",\n",
    "          \"t\": \"Computation Time [s]\"\n",
    "         }\n",
    ").round(2).to_latex(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "arbitrary-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"\"\"\n",
    "BaggingRegressor &     63.58 &   120.44 &    181.32 &     92.98 &     97.23 &                1159.0 \\\\\n",
    "               ExtraTreesRegressor &     64.19 &   120.60 &    184.64 &     92.55 &     97.01 &               14593.0 \\\\\n",
    "                 AdaBoostRegressor &      9.48 &   419.73 &    476.10 &     29.25 &     49.39 &                  37.0 \\\\\n",
    "             RandomForestRegressor &     65.40 &   116.12 &    176.15 &     93.40 &     97.49 &                9049.0 \\\\\n",
    "     HistGradientBoostingRegressor &     63.84 &   117.36 &    173.90 &     93.91 &     97.73 &                 284.0 \\\\\n",
    "     HistGradientBoostingRegressor\\_LAD &     65.96 &   116.03 &    176.77 &     93.17 &     97.36 &                 549.0 \\\\\n",
    "         GradientBoostingRegressor &     58.65 &   127.10 &    183.19 &     93.23 &     97.31 &                  69.0 \\\\\n",
    "     GradientBoostingRegressor\\_LAD &     61.72 &   125.11 &    185.25 &     92.43 &     97.00 &                  72.0 \\\\\n",
    "   GradientBoostingRegressor\\_Huber &     60.05 &   126.02 &    183.76 &     92.93 &     97.16 &                  75.0 \\\\\n",
    "GradientBoostingRegressor\\_Quantile &     24.11 &   220.51 &    262.42 &     75.23 &     94.22 &                  72.0 \\\\\n",
    "\"\"\"\n",
    "a = a.strip(\"\\n\").strip(\"\\\\\").split('\\\\\\n')\n",
    "\n",
    "for i, line in enumerate(a):\n",
    "    a[i] = a[i].replace(\" \", \"\").split(\"&\")\n",
    "results = pd.DataFrame(a, columns = [\"Variation\", \"% <2 min\", \"MAE\", \"RMSE\", \"% <5 min\", \"% <7 min\", \"t\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-realtor",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "grave-protection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingRegressor, LAD_h30_all, SIMPLE_VAL, h30\n",
      "{'name': 'HistGradientBoostingRegressor, LAD_h30_all, SIMPLE_VAL, h30', 'RMSE': 176.77051446327275, 'MAE': 116.03181408174824, '% <2 min': 65.96125541986555, '% <5 min': 93.17395282230797, '% <7 min': 97.36465253192252, 'time': '2021-04-27 10:41:27', 't': 107.48}\n",
      "HistGradientBoostingRegressor, LAD_h30_fs_ga, SIMPLE_VAL, h30\n",
      "{'name': 'HistGradientBoostingRegressor, LAD_h30_fs_ga, SIMPLE_VAL, h30', 'RMSE': 177.13968558032084, 'MAE': 116.35166062634399, '% <2 min': 65.70269302677116, '% <5 min': 93.32312343370857, '% <7 min': 97.37061935637854, 'time': '2021-04-27 10:42:34', 't': 67.37}\n",
      "HistGradientBoostingRegressor, LAD_h30_fs_google, SIMPLE_VAL, h30\n",
      "{'name': 'HistGradientBoostingRegressor, LAD_h30_fs_google, SIMPLE_VAL, h30', 'RMSE': 178.55976738348582, 'MAE': 117.2874986283325, '% <2 min': 65.73650503202195, '% <5 min': 92.96113608337643, '% <7 min': 97.25526075022873, 'time': '2021-04-27 10:42:55', 't': 20.65}\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import Ridge\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "X_cols_fs = [\n",
    "    'actype',\n",
    "    'depgnr',\n",
    "    'n_dep',\n",
    "    'dew',\n",
    "    'lightning',\n",
    "    'rvr5000_2000',\n",
    "    'plr1',\n",
    "    'plr2',\n",
    "    'local_mod',\n",
    "    'trwy_ext'\n",
    "]\n",
    "\n",
    "X_cols_google = [\n",
    "    \"depgnr\", \"actype\", \"trwy\", \"trwy_ext\", \"sid\", \n",
    "    \"n_dep\", \"wtc\", \"ptr2\", \"local_week\", \"plr1\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "regr = HistGradientBoostingRegressor(\n",
    "        random_state=0,\n",
    "        #verbose=1,\n",
    "        loss='least_absolute_deviation',\n",
    ")\n",
    "regr_name = \"HistGradientBoostingRegressor, LAD\"\n",
    "\n",
    "for name, X_cols_sel in [(\"all\", X_cols), (\"fs_ga\", X_cols_fs), (\"fs_google\", X_cols_google)]:\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocessor\", gen_preprocessor(X_cols_sel)),\n",
    "        (\"Densifier\", DenseTransformer()),\n",
    "        (\"regressor\", regr)\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    results.append(\n",
    "        model_eval(\n",
    "            model.predict(X_val), \n",
    "            y_val, \n",
    "            name=\"{}_h30_{}, SIMPLE_VAL, h30\".format(regr_name, name), \n",
    "            t=round(time.time()-t0, 2),\n",
    "        )\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-tissue",
   "metadata": {},
   "source": [
    "# Table: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "stainless-pacific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "                                         Variation &  \\% <2 min &  MAE [s] &  RMSE [s] &  \\% <5 min &  \\% <7 min &  Computation Time [s] \\\\\n",
      "\\midrule\n",
      "HistGradientBoostingRegressor, LAD\\_h30\\_all, SIM... &     65.96 &   116.03 &    176.77 &     93.17 &     97.36 &                107.48 \\\\\n",
      "HistGradientBoostingRegressor, LAD\\_h30\\_fs\\_ga, S... &     65.70 &   116.35 &    177.14 &     93.32 &     97.37 &                 67.37 \\\\\n",
      "HistGradientBoostingRegressor, LAD\\_h30\\_fs\\_googl... &     65.74 &   117.29 &    178.56 &     92.96 &     97.26 &                 20.65 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = pd.DataFrame(results)\n",
    "out['label'] = out['name'].str.split('_').str[0]\n",
    "\n",
    "print(\n",
    "out[\n",
    "    [\n",
    "        'name', \n",
    "        '% <2 min', \n",
    "        'MAE', \n",
    "        \"RMSE\", \n",
    "        '% <5 min', \n",
    "        '% <7 min', \n",
    "        't'\n",
    "    ]\n",
    "].rename(columns=\n",
    "         {\"name\": \"Variation\", \n",
    "          \"MAE\": \"MAE [s]\",\n",
    "          \"RMSE\": \"RMSE [s]\",\n",
    "          \"t\": \"Computation Time [s]\"\n",
    "         }\n",
    ").round(2).to_latex(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-medicare",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-gibson",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'Regressor__l2_regularization': 0.35, 'Regressor__learning_rate': 0.095, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 600}\n",
      "{'RMSE': 171.51697254679848, 'MAE': 111.45401435096669, '% <2 min': 67.58423167190422, '% <5 min': 93.90389434742829, '% <7 min': 97.67492740363578, 'time': '2021-04-30 21:53:11'}\n",
      "1 {'Regressor__l2_regularization': 0.35, 'Regressor__learning_rate': 0.095, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 700}\n",
      "{'RMSE': 170.94861684039503, 'MAE': 111.18142810023123, '% <2 min': 67.78113687895302, '% <5 min': 94.0132861291221, '% <7 min': 97.68686105254784, 'time': '2021-04-30 22:20:13'}\n",
      "2 {'Regressor__l2_regularization': 0.35, 'Regressor__learning_rate': 0.095, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 800}\n",
      "{'RMSE': 171.1236594302545, 'MAE': 111.3135503886543, '% <2 min': 67.73936910776085, '% <5 min': 93.95162894307649, '% <7 min': 97.65106010581168, 'time': '2021-04-30 22:50:46'}\n",
      "3 {'Regressor__l2_regularization': 0.35, 'Regressor__learning_rate': 0.1, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 600}\n",
      "{'RMSE': 171.46686987852718, 'MAE': 111.32257457260137, '% <2 min': 67.76522534707028, '% <5 min': 93.86809340069216, '% <7 min': 97.61923704204622, 'time': '2021-04-30 23:14:08'}\n",
      "4 {'Regressor__l2_regularization': 0.35, 'Regressor__learning_rate': 0.1, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 700}\n",
      "{'RMSE': 171.34293795061163, 'MAE': 111.43580423412031, '% <2 min': 67.75528063964357, '% <5 min': 93.96157365050321, '% <7 min': 97.65901587175306, 'time': '2021-04-30 23:40:55'}\n",
      "5 {'Regressor__l2_regularization': 0.35, 'Regressor__learning_rate': 0.1, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 800}\n",
      "{'RMSE': 171.27055406064582, 'MAE': 111.38973740014205, '% <2 min': 67.64986674092049, '% <5 min': 93.96754047495922, '% <7 min': 97.65901587175306, 'time': '2021-05-01 00:11:17'}\n",
      "6 {'Regressor__l2_regularization': 0.35, 'Regressor__learning_rate': 0.105, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 600}\n",
      "{'RMSE': 171.4210567906643, 'MAE': 111.12414633571065, '% <2 min': 67.81892676717452, '% <5 min': 93.92179482079636, '% <7 min': 97.63912645689963, 'time': '2021-05-01 00:32:07'}\n",
      "7 {'Regressor__l2_regularization': 0.35, 'Regressor__learning_rate': 0.105, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 700}\n",
      "{'RMSE': 171.2332401955208, 'MAE': 111.36404906175957, '% <2 min': 67.54843072516806, '% <5 min': 93.95560682604717, '% <7 min': 97.64310433987032, 'time': '2021-05-01 00:58:06'}\n",
      "8 {'Regressor__l2_regularization': 0.35, 'Regressor__learning_rate': 0.105, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 800}\n",
      "{'RMSE': 171.69039645426182, 'MAE': 111.58251113998173, '% <2 min': 67.60809896972832, '% <5 min': 93.91185011336967, '% <7 min': 97.61525915907553, 'time': '2021-05-01 01:20:37'}\n",
      "9 {'Regressor__l2_regularization': 0.375, 'Regressor__learning_rate': 0.095, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 600}\n",
      "{'RMSE': 171.3646350797505, 'MAE': 111.23200838404732, '% <2 min': 67.83483829905724, '% <5 min': 93.8939496400016, '% <7 min': 97.61923704204622, 'time': '2021-05-01 01:44:01'}\n",
      "10 {'Regressor__l2_regularization': 0.375, 'Regressor__learning_rate': 0.095, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 700}\n",
      "{'RMSE': 171.16623027932548, 'MAE': 111.22485556280014, '% <2 min': 67.7274354588488, '% <5 min': 93.98742988981265, '% <7 min': 97.68288316957715, 'time': '2021-05-01 02:10:42'}\n",
      "11 {'Regressor__l2_regularization': 0.375, 'Regressor__learning_rate': 0.095, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 800}\n",
      "{'RMSE': 171.37131323331536, 'MAE': 111.2331560072464, '% <2 min': 67.78511476192371, '% <5 min': 93.95759576753252, '% <7 min': 97.66697163769442, 'time': '2021-05-01 02:34:17'}\n",
      "12 {'Regressor__l2_regularization': 0.375, 'Regressor__learning_rate': 0.1, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 600}\n",
      "{'RMSE': 171.2774009899204, 'MAE': 111.2394467888575, '% <2 min': 67.63196626755241, '% <5 min': 93.91980587931104, '% <7 min': 97.62918174947292, 'time': '2021-05-01 02:57:22'}\n",
      "13 {'Regressor__l2_regularization': 0.375, 'Regressor__learning_rate': 0.1, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 700}\n",
      "{'RMSE': 171.30382731459704, 'MAE': 111.28278771532065, '% <2 min': 67.83483829905724, '% <5 min': 93.9754962409006, '% <7 min': 97.66896057917977, 'time': '2021-05-01 03:24:31'}\n",
      "14 {'Regressor__l2_regularization': 0.375, 'Regressor__learning_rate': 0.1, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 800}\n",
      "{'RMSE': 171.4398268726084, 'MAE': 111.44541350732462, '% <2 min': 67.73141334181948, '% <5 min': 93.8939496400016, '% <7 min': 97.66697163769442, 'time': '2021-05-01 03:54:58'}\n",
      "15 {'Regressor__l2_regularization': 0.375, 'Regressor__learning_rate': 0.105, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 600}\n",
      "{'RMSE': 171.50916291197598, 'MAE': 111.50831801221686, '% <2 min': 67.67174509725923, '% <5 min': 93.87207128366283, '% <7 min': 97.6132702175902, 'time': '2021-05-01 04:14:53'}\n",
      "16 {'Regressor__l2_regularization': 0.375, 'Regressor__learning_rate': 0.105, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 700}\n",
      "{'RMSE': 171.37373280256384, 'MAE': 111.5443012979009, '% <2 min': 67.68765662914197, '% <5 min': 93.97151835792991, '% <7 min': 97.62520386650225, 'time': '2021-05-01 04:41:31'}\n",
      "17 {'Regressor__l2_regularization': 0.375, 'Regressor__learning_rate': 0.105, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 800}\n",
      "{'RMSE': 171.1386863873331, 'MAE': 111.22767869129875, '% <2 min': 67.67373403874458, '% <5 min': 93.96157365050321, '% <7 min': 97.64111539838498, 'time': '2021-05-01 05:11:54'}\n",
      "18 {'Regressor__l2_regularization': 0.4, 'Regressor__learning_rate': 0.095, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 600}\n",
      "{'RMSE': 171.4720049022047, 'MAE': 111.38099307879183, '% <2 min': 67.63395520903775, '% <5 min': 93.93969529416445, '% <7 min': 97.62122598353157, 'time': '2021-05-01 05:35:05'}\n",
      "19 {'Regressor__l2_regularization': 0.4, 'Regressor__learning_rate': 0.095, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 700}\n",
      "{'RMSE': 171.3826257359938, 'MAE': 111.48360842172016, '% <2 min': 67.69561239508334, '% <5 min': 93.9377063526791, '% <7 min': 97.65106010581168, 'time': '2021-05-01 06:02:11'}\n",
      "20 {'Regressor__l2_regularization': 0.4, 'Regressor__learning_rate': 0.095, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 800}\n",
      "{'RMSE': 170.96758101632548, 'MAE': 111.0652464261985, '% <2 min': 67.77914793746767, '% <5 min': 94.05107601734358, '% <7 min': 97.65304904729703, 'time': '2021-05-01 06:31:56'}\n",
      "21 {'Regressor__l2_regularization': 0.4, 'Regressor__learning_rate': 0.1, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 600}\n",
      "{'RMSE': 171.41104592724255, 'MAE': 111.3836360859831, '% <2 min': 67.72942440033414, '% <5 min': 93.913839054855, '% <7 min': 97.63117069095827, 'time': '2021-05-01 06:55:19'}\n",
      "22 {'Regressor__l2_regularization': 0.4, 'Regressor__learning_rate': 0.1, 'Regressor__max_iter': 200, 'Regressor__max_leaf_nodes': 700}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, \\\n",
    "    BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = {\n",
    "#     'Regressor__l2_regularization': [0.350, .375, 0.4],\n",
    "#     'Regressor__learning_rate': [0.095, 0.1, 0.105],\n",
    "#     'Regressor__max_leaf_nodes': [600, 700, 800],\n",
    "#     'Regressor__max_leaf_nodes': [75, 100, 125],\n",
    "    'Regressor__max_leaf_nodes': [100, 125, 200],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "i= 0\n",
    "for g in list(ParameterGrid(grid)) + [{'Regressor__l2_regularization': 0.4, 'Regressor__learning_rate': 0.1, 'Regressor__max_leaf_nodes': 900}]:\n",
    "# for g in ParameterGrid(grid):\n",
    "    print(i, g)\n",
    "    i += 1\n",
    "    regr = HistGradientBoostingRegressor(\n",
    "        loss='least_absolute_deviation',\n",
    "        random_state=0,\n",
    "        #verbose=1,\n",
    "    )\n",
    "    \n",
    "    model = Pipeline(steps=[\n",
    "        (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "        (\"Densifier\", DenseTransformer()),\n",
    "        (\"Regressor\", regr),\n",
    "    ])\n",
    "    \n",
    "    model.set_params(**g)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    scores = model_eval(y_val, model.predict(X_val))\n",
    "    results.append(scores.update(g))\n",
    "    \n",
    "    if scores['% <2 min'] > best_score:\n",
    "        best_score = scores['% <2 min']\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "crucial-strap",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>% &lt;2 min</th>\n",
       "      <th>% &lt;5 min</th>\n",
       "      <th>% &lt;7 min</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>171.143664</td>\n",
       "      <td>110.957130</td>\n",
       "      <td>67.958153</td>\n",
       "      <td>93.969529</td>\n",
       "      <td>97.649071</td>\n",
       "      <td>2021-04-29 17:03:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171.529833</td>\n",
       "      <td>111.176173</td>\n",
       "      <td>67.912407</td>\n",
       "      <td>93.935717</td>\n",
       "      <td>97.635149</td>\n",
       "      <td>2021-04-29 16:12:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171.277463</td>\n",
       "      <td>111.097685</td>\n",
       "      <td>67.882573</td>\n",
       "      <td>93.945662</td>\n",
       "      <td>97.643104</td>\n",
       "      <td>2021-04-29 15:55:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>171.518005</td>\n",
       "      <td>111.291858</td>\n",
       "      <td>67.872628</td>\n",
       "      <td>93.923784</td>\n",
       "      <td>97.617248</td>\n",
       "      <td>2021-04-29 17:20:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>171.535257</td>\n",
       "      <td>111.324571</td>\n",
       "      <td>67.850750</td>\n",
       "      <td>93.905883</td>\n",
       "      <td>97.633160</td>\n",
       "      <td>2021-04-29 16:29:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171.179828</td>\n",
       "      <td>111.120287</td>\n",
       "      <td>67.824894</td>\n",
       "      <td>93.939695</td>\n",
       "      <td>97.635149</td>\n",
       "      <td>2021-04-29 15:21:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171.543162</td>\n",
       "      <td>111.257616</td>\n",
       "      <td>67.806993</td>\n",
       "      <td>93.856160</td>\n",
       "      <td>97.619237</td>\n",
       "      <td>2021-04-29 15:05:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171.372474</td>\n",
       "      <td>111.192316</td>\n",
       "      <td>67.781137</td>\n",
       "      <td>93.939695</td>\n",
       "      <td>97.651060</td>\n",
       "      <td>2021-04-29 15:38:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>171.601774</td>\n",
       "      <td>111.353500</td>\n",
       "      <td>67.777159</td>\n",
       "      <td>93.874060</td>\n",
       "      <td>97.603326</td>\n",
       "      <td>2021-04-29 16:46:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RMSE         MAE   % <2 min   % <5 min   % <7 min  \\\n",
       "7  171.143664  110.957130  67.958153  93.969529  97.649071   \n",
       "4  171.529833  111.176173  67.912407  93.935717  97.635149   \n",
       "3  171.277463  111.097685  67.882573  93.945662  97.643104   \n",
       "8  171.518005  111.291858  67.872628  93.923784  97.617248   \n",
       "5  171.535257  111.324571  67.850750  93.905883  97.633160   \n",
       "1  171.179828  111.120287  67.824894  93.939695  97.635149   \n",
       "0  171.543162  111.257616  67.806993  93.856160  97.619237   \n",
       "2  171.372474  111.192316  67.781137  93.939695  97.651060   \n",
       "6  171.601774  111.353500  67.777159  93.874060  97.603326   \n",
       "\n",
       "                  time  \n",
       "7  2021-04-29 17:03:05  \n",
       "4  2021-04-29 16:12:21  \n",
       "3  2021-04-29 15:55:26  \n",
       "8  2021-04-29 17:20:08  \n",
       "5  2021-04-29 16:29:29  \n",
       "1  2021-04-29 15:21:59  \n",
       "0  2021-04-29 15:05:06  \n",
       "2  2021-04-29 15:38:33  \n",
       "6  2021-04-29 16:46:20  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(results).sort_values('% <2 min', ascending=False)\n",
    "\n",
    "best_grid = {\n",
    "    'Regressor__l2_regularization': 0.375,\n",
    "    'Regressor__learning_rate': 0.1,\n",
    "    'Regressor__max_leaf_nodes': 700,\n",
    "}\n",
    "best_score =     \n",
    "\n",
    "print(best_score)\n",
    "print(best_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     'Regressor__l2_regularization': [0.0, 0.5, 1.5],\n",
    "#     'Regressor__learning_rate': [0.01, 0.1, 1],\n",
    "#     'Regressor__max_leaf_nodes': [25, 31, 40],\n",
    "#     'Regressor__validation_fraction': [0.05, 0.1, 0.15],\n",
    "#     'Regressor__l2_regularization': [0.25, 0.5, 1],\n",
    "#     'Regressor__learning_rate': [0.05, 0.1, 0.5],\n",
    "#     'Regressor__max_leaf_nodes': [36, 40, 50],\n",
    "#     'Regressor__validation_fraction': [0.08, 0.1, 0.12],\n",
    "# Stopped Early: max leaf none reduced performance while requiring two hours computation time.\n",
    "#     'Regressor__l2_regularization': [0.4, 0.5, 0.6],\n",
    "#     'Regressor__learning_rate': [0.08, 0.1, 0.2],\n",
    "#     'Regressor__max_leaf_nodes': [50, 100, None],\n",
    "# Continued\n",
    "#     'Regressor__l2_regularization': [0.4, 0.5, 0.6],\n",
    "#     'Regressor__learning_rate': [0.08, 0.1, 0.2],\n",
    "#     'Regressor__max_leaf_nodes': [50, 100], \n",
    "#     'Regressor__l2_regularization': [0.35, 0.4, 0.45],\n",
    "#     'Regressor__learning_rate': [0.09, 0.1, 0.12],\n",
    "#     'Regressor__max_leaf_nodes': [100, 150],\n",
    "#     'Regressor__l2_regularization': [0.35, .375, 0.4],\n",
    "#     'Regressor__learning_rate': [0.09, 0.095, 0.1],\n",
    "#     'Regressor__max_leaf_nodes': [400],\n",
    "#     'Regressor__l2_regularization': [0.35, .375, 0.4],\n",
    "#     'Regressor__learning_rate': [0.09, 0.095, 0.1],\n",
    "#     'Regressor__max_leaf_nodes': [800],\n",
    "#     'Regressor__l2_regularization': [0.35, .375, 0.4],\n",
    "#     'Regressor__learning_rate': [0.095],\n",
    "#     'Regressor__max_leaf_nodes': [800],\n",
    "#     'Regressor__l2_regularization': [0.375, 0.4,0.425],\n",
    "#     'Regressor__learning_rate': [0.095],\n",
    "#     'Regressor__max_leaf_nodes': [1200],\n",
    "#     'Regressor__l2_regularization': [0.375, 0.4, 0.425],\n",
    "#     'Regressor__learning_rate': [0.095, 0.1],\n",
    "#     'Regressor__max_leaf_nodes': [700, 800, 900],\n",
    "#     'Regressor__l2_regularization': [0.375,],\n",
    "#     'Regressor__learning_rate': [0.1],\n",
    "#     'Regressor__max_leaf_nodes': [700],\n",
    "#     'Regressor__max_iter': [100, 200, 400, 800]\n",
    "#     'Regressor__l2_regularization': [0.35, 0.375,0.4],\n",
    "#     'Regressor__learning_rate': [0.095, 0.1, 0.105],\n",
    "#     'Regressor__max_leaf_nodes': [600, 700, 800],\n",
    "#     'Regressor__max_iter': [200]\n",
    "\n",
    "# best_grid = {\n",
    "#     'Regressor__l2_regularization': 0.5,\n",
    "#     'Regressor__learning_rate': 0.1,,\n",
    "#     'Regressor__max_leaf_nodes': 40,\n",
    "#     'Regressor__validation_fraction': 0.1,\n",
    "# }\n",
    "# best_grid = {\n",
    "#     'Regressor__l2_regularization': 1,\n",
    "#     'Regressor__learning_rate': 0.1,\n",
    "#     'Regressor__max_leaf_nodes': 50,\n",
    "#     'Regressor__validation_fraction': 0.08,\n",
    "# }\n",
    "# best_grid = {\n",
    "#     'Regressor__l2_regularization': 0.4,\n",
    "#     'Regressor__learning_rate': 0.1,\n",
    "#     'Regressor__max_leaf_nodes': 100,\n",
    "# }\n",
    "# best_grid = {\n",
    "#     'Regressor__l2_regularization': 0.4,\n",
    "#     'Regressor__learning_rate': 0.1,\n",
    "#     'Regressor__max_leaf_nodes': 100,\n",
    "# }\n",
    "# {'Regressor__l2_regularization': 0.35,\n",
    "#  'Regressor__learning_rate': 0.09,\n",
    "#  'Regressor__max_leaf_nodes': 150}\n",
    "\n",
    "# {'Regressor__l2_regularization': 0.375,\n",
    "#  'Regressor__learning_rate': 0.095,\n",
    "#  'Regressor__max_leaf_nodes': 200}\n",
    "# {'Regressor__l2_regularization': 0.375,\n",
    "#  'Regressor__learning_rate': 0.095,\n",
    "#  'Regressor__max_leaf_nodes': 250}\n",
    "# {'Regressor__l2_regularization': 0.375,\n",
    "#  'Regressor__learning_rate': 0.09,\n",
    "#  'Regressor__max_leaf_nodes': 400}\n",
    "# {'Regressor__l2_regularization': 0.4,\n",
    "#  'Regressor__learning_rate': 0.095,\n",
    "#  'Regressor__max_leaf_nodes': 800}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-harvest",
   "metadata": {},
   "source": [
    "# Final model Evaluation on Valdiation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "compound-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "rural-maine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_not_tuned_val\n",
      "{'name': 'rf_not_tuned_val', 'RMSE': 176.77051446327275, 'MAE': 116.03181408174824, '% <2 min': 65.96125541986555, '% <5 min': 93.17395282230797, '% <7 min': 97.36465253192252, 'time': '2021-05-01 07:47:02', 't': 113.65787887573242}\n",
      "rf_tuned_val\n",
      "{'name': 'rf_tuned_val', 'RMSE': 171.49902960585467, 'MAE': 111.14193791865826, '% <2 min': 68.04566609650344, '% <5 min': 93.90986117188433, '% <7 min': 97.62918174947292, 'time': '2021-05-01 08:01:48', 't': 885.9712924957275}\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT\n",
    "t0 = time.time()\n",
    "regr = HistGradientBoostingRegressor(\n",
    "    loss='least_absolute_deviation',\n",
    "    random_state=0,\n",
    "    #verbose=1,\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "    (\"Densifier\", DenseTransformer()),\n",
    "    (\"Regressor\", regr),\n",
    "])\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "results.append(\n",
    "    model_eval(\n",
    "        y_val, \n",
    "        model.predict(X_val),\n",
    "        name = \"rf_not_tuned_val\",\n",
    "        t = time.time()-t0,\n",
    "    )\n",
    ")\n",
    "\n",
    "# TUNED\n",
    "t0 = time.time()\n",
    "\n",
    "regr = HistGradientBoostingRegressor(\n",
    "    loss='least_absolute_deviation',\n",
    "    random_state=0,\n",
    "    #verbose=1,\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "    (\"Densifier\", DenseTransformer()),\n",
    "    (\"Regressor\", regr),\n",
    "])\n",
    "\n",
    "tuned_params = {\n",
    "    'Regressor__l2_regularization': 0.375, \n",
    "    'Regressor__learning_rate': 0.1, \n",
    "    'Regressor__max_leaf_nodes': 700\n",
    "}\n",
    "\n",
    "model.set_params(**tuned_params)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "results.append(\n",
    "    model_eval(\n",
    "        y_val, \n",
    "        model.predict(X_val),\n",
    "        name=\"rf_tuned_val\",\n",
    "        t = time.time()-t0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "color-toddler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "       Variation &  \\% <2 min &  MAE [s] &  RMSE [s] &  \\% <5 min &  \\% <7 min &  Computation Time [s] \\\\\n",
      "\\midrule\n",
      "             NaN &     65.96 &   116.03 &    176.77 &     93.17 &     97.36 &                   NaN \\\\\n",
      "rf\\_not\\_tuned\\_val &     65.96 &   116.03 &    176.77 &     93.17 &     97.36 &                   NaN \\\\\n",
      "    rf\\_tuned\\_val &     68.05 &   111.14 &    171.50 &     93.91 &     97.63 &                   NaN \\\\\n",
      "rf\\_not\\_tuned\\_val &     65.96 &   116.03 &    176.77 &     93.17 &     97.36 &                113.66 \\\\\n",
      "    rf\\_tuned\\_val &     68.05 &   111.14 &    171.50 &     93.91 &     97.63 &                885.97 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "pd.DataFrame(results)[\n",
    "    [\n",
    "        'name', \n",
    "        '% <2 min', \n",
    "        'MAE', \n",
    "        \"RMSE\", \n",
    "        '% <5 min', \n",
    "        '% <7 min', \n",
    "        't'\n",
    "    ]\n",
    "].rename(columns=\n",
    "         {\"name\": \"Variation\", \n",
    "          \"MAE\": \"MAE [s]\",\n",
    "          \"RMSE\": \"RMSE [s]\",\n",
    "          \"t\": \"Computation Time [s]\"\n",
    "         }\n",
    ").round(2).to_latex(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-provider",
   "metadata": {},
   "source": [
    "# Final Model Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "impossible-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "quantitative-lease",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RMSE': 164.80658425276954, 'MAE': 115.65055934050312, '% <2 min': 64.98001074049765, '% <5 min': 93.65515046641606, '% <7 min': 97.65698032897745, 'time': '2021-05-01 08:30:33', 't': 121.47571873664856}\n",
      "{'RMSE': 158.51600726205046, 'MAE': 110.78240462204197, '% <2 min': 66.83573005549258, '% <5 min': 94.46068778964538, '% <7 min': 97.99510710662928, 'time': '2021-05-01 08:45:58', 't': 924.2915828227997}\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT\n",
    "t0 = time.time()\n",
    "\n",
    "regr = HistGradientBoostingRegressor(\n",
    "    loss='least_absolute_deviation',\n",
    "    random_state=0,\n",
    "    #verbose=1,\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "    (\"Densifier\", DenseTransformer()),\n",
    "    (\"Regressor\", regr),\n",
    "])\n",
    "\n",
    "model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))\n",
    "\n",
    "results.append(\n",
    "    model_eval(\n",
    "        y_test, \n",
    "        model.predict(X_test),\n",
    "        t = time.time()-t0,\n",
    "    )\n",
    ")\n",
    "\n",
    "# TUNED\n",
    "t0 = time.time()\n",
    "\n",
    "regr = HistGradientBoostingRegressor(\n",
    "    loss='least_absolute_deviation',\n",
    "    random_state=0,\n",
    "    #verbose=1,\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "    (\"Densifier\", DenseTransformer()),\n",
    "    (\"Regressor\", regr),\n",
    "])\n",
    "\n",
    "tuned_params = {\n",
    "    'Regressor__l2_regularization': 0.375, \n",
    "    'Regressor__learning_rate': 0.1, \n",
    "    'Regressor__max_leaf_nodes': 700\n",
    "}\n",
    "\n",
    "model.set_params(**tuned_params)\n",
    "\n",
    "model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))\n",
    "\n",
    "results.append(\n",
    "    model_eval(\n",
    "        y_test, \n",
    "        model.predict(X_test),\n",
    "        t = time.time()-t0,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-handling",
   "metadata": {},
   "source": [
    "# Table: I don't remember\n",
    "\n",
    "Last line appears to be the performance of the tuned model on the test data used in model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "central-allocation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrrr}\n",
      "\\toprule\n",
      " \\% <2 min &  MAE [s] &  RMSE [s] &  \\% <5 min &  \\% <7 min &  Computation Time [s] \\\\\n",
      "\\midrule\n",
      "    64.98 &   115.65 &    164.81 &     93.66 &     97.66 &                   NaN \\\\\n",
      "    64.98 &   115.65 &    164.81 &     93.66 &     97.66 &                   NaN \\\\\n",
      "    66.84 &   110.78 &    158.52 &     94.46 &     98.00 &                   NaN \\\\\n",
      "    64.98 &   115.65 &    164.81 &     93.66 &     97.66 &                123.36 \\\\\n",
      "    64.98 &   115.65 &    164.81 &     93.66 &     97.66 &                121.48 \\\\\n",
      "    66.84 &   110.78 &    158.52 &     94.46 &     98.00 &                924.29 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "pd.DataFrame(results)[\n",
    "    [\n",
    "        #'name', \n",
    "        '% <2 min', \n",
    "        'MAE', \n",
    "        \"RMSE\", \n",
    "        '% <5 min', \n",
    "        '% <7 min', \n",
    "        't'\n",
    "    ]\n",
    "].rename(\n",
    "    columns= {\n",
    "        #\"name\": \"Variation\", \n",
    "        \"MAE\": \"MAE [s]\",\n",
    "        \"RMSE\": \"RMSE [s]\",\n",
    "        \"t\": \"Computation Time [s]\"\n",
    "         }\n",
    ").round(2).to_latex(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-denver",
   "metadata": {},
   "source": [
    "# Evaluation Final Tree Based Ensemble (tbe.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "artificial-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "honey-sugar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGBR_h30_final\n",
      "{'name': 'HGBR_h30_final', 'RMSE': 155.29769154285586, 'MAE': 109.76167849998207, '% <2 min': 66.18116876674274, '% <5 min': 95.14634388332176, '% <7 min': 98.31134041075504, 'time': '2021-05-01 09:11:36', 'dataset_train': 'processed_dep_h30', 'dataset_test': 'processed_dep_h0', 'model_type': 'Tree-Based_Ensemble_Model', 'eval_type': 'SIMPLE_TEST'}\n",
      "HGBR_h30_final\n",
      "{'name': 'HGBR_h30_final', 'RMSE': 219.7015064695633, 'MAE': 156.9140452019531, '% <2 min': 52.83754160915808, '% <5 min': 85.83664853454574, '% <7 min': 93.55362507104003, 'time': '2021-05-01 09:11:41', 'dataset_train': 'processed_dep_h30', 'dataset_test': 'processed_dep_h120', 'model_type': 'Tree-Based_Ensemble_Model', 'eval_type': 'SIMPLE_TEST'}\n",
      "HGBR_h30_final\n",
      "{'name': 'HGBR_h30_final', 'RMSE': 214.96725102916685, 'MAE': 153.7823610548063, '% <2 min': 53.54300077802382, '% <5 min': 86.31276557543839, '% <7 min': 94.00622419055598, 'time': '2021-05-01 09:11:48', 'dataset_train': 'processed_dep_h30', 'dataset_test': 'processed_dep_h180', 'model_type': 'Tree-Based_Ensemble_Model', 'eval_type': 'SIMPLE_TEST'}\n",
      "HGBR_h30_final\n",
      "{'name': 'HGBR_h30_final', 'RMSE': 158.51600726205046, 'MAE': 110.78240462204197, '% <2 min': 66.83573005549258, '% <5 min': 94.46068778964538, '% <7 min': 97.99510710662928, 'time': '2021-05-01 09:11:50', 'dataset_train': 'processed_dep_h30', 'dataset_test': 'processed_dep_h30', 'model_type': 'Tree-Based_Ensemble_Model', 'eval_type': 'SIMPLE_TEST'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>% &lt;2 min</th>\n",
       "      <th>% &lt;5 min</th>\n",
       "      <th>% &lt;7 min</th>\n",
       "      <th>time</th>\n",
       "      <th>dataset_train</th>\n",
       "      <th>dataset_test</th>\n",
       "      <th>model_type</th>\n",
       "      <th>eval_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HGBR_h30_final</td>\n",
       "      <td>155.297692</td>\n",
       "      <td>109.761678</td>\n",
       "      <td>66.181169</td>\n",
       "      <td>95.146344</td>\n",
       "      <td>98.311340</td>\n",
       "      <td>2021-05-01 09:11:36</td>\n",
       "      <td>processed_dep_h30</td>\n",
       "      <td>processed_dep_h0</td>\n",
       "      <td>Tree-Based_Ensemble_Model</td>\n",
       "      <td>SIMPLE_TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HGBR_h30_final</td>\n",
       "      <td>219.701506</td>\n",
       "      <td>156.914045</td>\n",
       "      <td>52.837542</td>\n",
       "      <td>85.836649</td>\n",
       "      <td>93.553625</td>\n",
       "      <td>2021-05-01 09:11:41</td>\n",
       "      <td>processed_dep_h30</td>\n",
       "      <td>processed_dep_h120</td>\n",
       "      <td>Tree-Based_Ensemble_Model</td>\n",
       "      <td>SIMPLE_TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HGBR_h30_final</td>\n",
       "      <td>214.967251</td>\n",
       "      <td>153.782361</td>\n",
       "      <td>53.543001</td>\n",
       "      <td>86.312766</td>\n",
       "      <td>94.006224</td>\n",
       "      <td>2021-05-01 09:11:48</td>\n",
       "      <td>processed_dep_h30</td>\n",
       "      <td>processed_dep_h180</td>\n",
       "      <td>Tree-Based_Ensemble_Model</td>\n",
       "      <td>SIMPLE_TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HGBR_h30_final</td>\n",
       "      <td>158.516007</td>\n",
       "      <td>110.782405</td>\n",
       "      <td>66.835730</td>\n",
       "      <td>94.460688</td>\n",
       "      <td>97.995107</td>\n",
       "      <td>2021-05-01 09:11:50</td>\n",
       "      <td>processed_dep_h30</td>\n",
       "      <td>processed_dep_h30</td>\n",
       "      <td>Tree-Based_Ensemble_Model</td>\n",
       "      <td>SIMPLE_TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name        RMSE         MAE   % <2 min   % <5 min   % <7 min  \\\n",
       "0  HGBR_h30_final  155.297692  109.761678  66.181169  95.146344  98.311340   \n",
       "1  HGBR_h30_final  219.701506  156.914045  52.837542  85.836649  93.553625   \n",
       "2  HGBR_h30_final  214.967251  153.782361  53.543001  86.312766  94.006224   \n",
       "3  HGBR_h30_final  158.516007  110.782405  66.835730  94.460688  97.995107   \n",
       "\n",
       "                  time      dataset_train        dataset_test  \\\n",
       "0  2021-05-01 09:11:36  processed_dep_h30    processed_dep_h0   \n",
       "1  2021-05-01 09:11:41  processed_dep_h30  processed_dep_h120   \n",
       "2  2021-05-01 09:11:48  processed_dep_h30  processed_dep_h180   \n",
       "3  2021-05-01 09:11:50  processed_dep_h30   processed_dep_h30   \n",
       "\n",
       "                  model_type    eval_type  \n",
       "0  Tree-Based_Ensemble_Model  SIMPLE_TEST  \n",
       "1  Tree-Based_Ensemble_Model  SIMPLE_TEST  \n",
       "2  Tree-Based_Ensemble_Model  SIMPLE_TEST  \n",
       "3  Tree-Based_Ensemble_Model  SIMPLE_TEST  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = HistGradientBoostingRegressor(\n",
    "    loss='least_absolute_deviation',\n",
    "    random_state=0,\n",
    "    #verbose=1,\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"Preprocessor\", gen_preprocessor(X_cols)),\n",
    "    (\"Densifier\", DenseTransformer()),\n",
    "    (\"Regressor\", regr),\n",
    "])\n",
    "\n",
    "tuned_params = {\n",
    "    'Regressor__l2_regularization': 0.375, \n",
    "    'Regressor__learning_rate': 0.1, \n",
    "    'Regressor__max_leaf_nodes': 700\n",
    "}\n",
    "\n",
    "model.set_params(**tuned_params)\n",
    "\n",
    "model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))\n",
    "\n",
    "for h in [0, 120, 180, 30,]:\n",
    "    df = data['processed_dep_h{}'.format(h)]\n",
    "\n",
    "    X_test = df[df['dtype']==\"TEST\"]\n",
    "    X_test.pop(\"dtype\")\n",
    "    y_test = X_test.pop(\"t_taxi\")\n",
    "\n",
    "    results.append(\n",
    "        model_eval(\n",
    "            y_test, \n",
    "            model.predict(X_test),\n",
    "            name=\"HGBR_h30_final\",\n",
    "            dataset_train=\"processed_dep_h30\",\n",
    "            dataset_test=\"processed_dep_h{}\".format(h),\n",
    "            model_type=\"Tree-Based_Ensemble_Model\",\n",
    "            eval_type=\"SIMPLE_TEST\",\n",
    "        )\n",
    "    )\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "smart-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv(\"../results/model_rf_tuned.csv\", index=False)\n",
    "import json\n",
    "with open(\"../results/model_rf_tuned.results\", 'w') as f:\n",
    "    #json.dump(results, f)\n",
    "    f.write(json.dumps(results)[1:-1].replace(\"}, {\", \"}\\n{\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
